{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Zendesk Tickets Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Obtaining data from APIs](#first-bullet)\n",
    "\n",
    "* [Preprocessing](#second-bullet)\n",
    "\n",
    "* [WordCloud](#third-bullet)\n",
    "\n",
    "* [CountVectorizer](#fourth-bullet)\n",
    "\n",
    "* [LDA Model](#fifth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "WORKING"
    ]
   },
   "source": [
    "Set up the development environment: \n",
    "- Install Git\n",
    "- Install Anaconda distribution (bookmarks)\n",
    "\n",
    "- Install Node.js\n",
    "- Install Postman native app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "source": [
    "Create new Python environment and check if we are in the right environment:\n",
    "- create the new environment without asking for confirmation (conda create --name testenv pandas jupyter ipykernel tabulate -y)\n",
    "- activate the environment (conda activate testenv)\n",
    "- use ipykernel to register your new environment as a kernel named testenv (python -m ipykernel install --user --name testenv --display-name \"testenv\")\n",
    "- import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "source": [
    "In the new environment install the following packages:\n",
    "- Install ijson (conda install)\n",
    "- Install natsort (conda install)\n",
    "- Install package gensim (pip install)\n",
    "- Install package spacy (conda install)\n",
    "- Download package en-core-web-sm (python -m spacy download en)\n",
    "- Install matplotlib (conda install)\n",
    "- Install package sklearn (pip install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "source": [
    "In the test environment for jupyter unresponsive error install the following: \n",
    "- Install package nbstripout (pip install nbstripout)\n",
    "- To clear the error: in terminal nbstripout filename.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file diagrams/block_diagram\n",
    "blockdiag {\n",
    "    orientation = portrait\n",
    "\n",
    "  A -> B -> C -> D -> E -> F;\n",
    "\n",
    "  // Set labels to nodes.\n",
    "   A [label = \"Obtaining data from APIs\"];\n",
    "   B [label = \"Preprocessing\"];\n",
    "   C [label = \"WordCloud\"];\n",
    "   D [label = \"CountVectorizer\"];\n",
    "   E [label = \"LDA Model\"];\n",
    "   F [label = \"Test\"];\n",
    " \n",
    "  // Set boder-style, background-color and text-color to nodes.\n",
    "  \n",
    "   A,B,C,D,E,F [color = \"#31708f\", textcolor=\"#FFFFFF\"];\n",
    "\n",
    "  // Set width and height to nodes.\n",
    "   A,B,C,D,E,F [width = 300]; // default value is 128\n",
    "    \n",
    "  // Set fontsize\n",
    "  default_fontsize = 20;  // default value is 11\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!blockdiag diagrams/block_diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"diagrams/block_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up jekyll for gh pages:\n",
    "- Install Ruby (verify installation: ruby -v; gem -v)\n",
    "- Install jekyll bundler (gem install; verify installation: jekyll -v)\n",
    "- cd Documents (in git bash)\n",
    "- jekyll new mynewsite\n",
    "- cd mynewsite\n",
    "- bundle exec jekyll serve\n",
    "- gem \"jekyll-theme-hydeout\" (in the gemfile)\n",
    "- bundle install (in git bash)\n",
    "- theme: jekyll-theme-hydeout (in _config file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test environment for word cloud install the following: \n",
    "- Install package wordcloud (conda install -c conda-forge wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test environment for word cloud TF-IDF install the following:\n",
    "- Install package numpy (conda install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new environment for topic modelling install the following:\n",
    "- Install package pandas (conda install)\n",
    "- Install package nltk (conda install)\n",
    "\n",
    "\n",
    "\n",
    "- Install package pyLDAvis (pip install)\n",
    "- Install package scipy (conda install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test environment for block diagram install the following:\n",
    "- Install package blockdiag (pip install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a package is installed:\n",
    "- In Anaconda Prompt (run as Admin)\n",
    "- conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Anaconda Prompt</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of all my environments, active environment is shown with *. It will display the base environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new environment named testenv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n testenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the new environment created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Jupyter Lab: http://localhost:8888/lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Obtaining data from APIs<a class=\"anchor\" id=\"first-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">In Git Bash</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Desktop/JUPYTER_NOTEBOOKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install Newman (command-line collection runner for Postman):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npm install newman -global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Base64</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTHORIZATION - Basic Authentication and API tokens<br> \n",
    "Use your company email address and Zendesk API key. The credentials must be sent in an Authorization header in the HTTP request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate a request with basic authentication and API token:\n",
    "- Combine your email address/token with your Zendesk API key with a colon:\n",
    "```svetlana.staneva@eventsforce.com/token: {zendesk_api_key}```\n",
    "- Base64-encode the resulting string:\n",
    "```amRvZUBleGFtcGxlLmNvbTpwYSQkdzByZA==```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In TextMechanic<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate a lists of numbers (1-3000, 3001-5000, etc.) up to 59046\n",
    "- Create a runner.csv files with a column ticket_id and listing numbers from 1 to 3000, etc.\n",
    "- Move the files to the working directory JUPYTER_NOTEBOOKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Postman<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postman Comments Collection<br>\n",
    "The Comments is a Postman collection that lists comments for Zendesk tickets from number 1 to 59046."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new environment:\n",
    "- Click on the Cog icon\n",
    "- Click Add\n",
    "- In Add Environment enter a name for the environment - for instance, Test \n",
    "- Click Add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an active environment:\n",
    "- Click the dropdown menu in the upper right corner of the Postman app to select an active environment (Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request data via GET request: https://eventsforce.zendesk.com/api/v2/tickets/{ticket_id}/comments.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Headers include the base64-encoded string:\n",
    "- In Headers go to Presets > Manage Presets\n",
    "- Add the Authorization and click Add:\n",
    "```Authorization: Basic {base64-encoded-string}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tests add the following line of code:\n",
    "```tests[responseBody] = true;```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Comments Collection\n",
    "- Click on Collections > Create New Collection\n",
    "- Enter a name - for instance, Comments and click Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Request in the Collection:\n",
    "- Click on Save As\n",
    "- Select the Collection and click Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the collection locally in the Postman Collection Runner with the runner_head.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the Comments collection: \n",
    "- Click on Collections \n",
    "- Hover over the Comments collection and click on the dots '...' that appear\n",
    "- Click Export and again Export\n",
    "- Save the file on the Desktop\n",
    "- The exported file is COMMENTS.postman_collection.json\n",
    "- Move the file COMMENTS.postman_collection.json to the working directory JUPYTER_NOTEBOOKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Git Bash</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Comments collection with the additional runner.csv file of key values (by 3000) and generate report in json.<br>\n",
    "```newman run COMMENTS.postman_collection.json -d runner.csv -r cli,json```<br>\n",
    "In case it gives an error 'JavaScript heap out of memory' run the code in the following format:<br>\n",
    "```NODE_OPTIONS=--max_old_space_size=2048 newman run COMMENTS.postman_collection.json -d runner.csv -r cli,json```<br>\n",
    "or<br>\n",
    "```NODE_OPTIONS=--max_old_space_size=3072 newman run COMMENTS.postman_collection.json -d runner.csv -r cli,json```<br>\n",
    "or run the Comments collection with the additional runner.csv file of key values (by 1000) and generate report in json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run multiple runs with different runner.csv files, create a name.sh file listing the commands:<br>\n",
    "```newman run COMMENTS.postman_collection.json -d runner_1000.csv -r cli,json```<br>\n",
    "```newman run COMMENTS.postman_collection.json -d runner_2000.csv -r cli,json```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the .sh file:<br>\n",
    "```bash name.sh```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Jupyter Notebook</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the working environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Jupyter Notebook</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Preprocessing<a class=\"anchor\" id=\"second-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "#Make sure glob.glob returns a list of files. List all json files in newman folder.\n",
    "file_list = glob.glob('/home/smsta/Desktop/zendesk_tag_cloud/newman/*.json')\n",
    "for filename in file_list:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "def parse_json(json_filename):\n",
    "        with open(filename, 'r', encoding=\"utf8\") as file:\n",
    "            # load json iteratively\n",
    "            parser = ijson.parse(file)\n",
    "            for prefix, event, value in parser:\n",
    "                print('prefix={}, event={}, value={}'.format(prefix, event, value))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parse_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "from string import punctuation\n",
    "def extract_ticket_text_generator(json_filename):\n",
    "    \"\"\"This function takes a list of files with tickets and extracts text from each ticket. The result is a list of text strings.\"\"\"\n",
    "    for filename in file_list:\n",
    "            # When you open the file specify the encoding.\n",
    "            with open(filename, 'r', encoding=\"utf8\") as input_file:\n",
    "                # Extract specific items from the file\n",
    "                tickets = ijson.items(input_file, 'run.executions.item.assertions.item.assertion')\n",
    "                for ticket in tickets:\n",
    "                   # Extract the substring between two markers\n",
    "                    l = re.findall('plain_body(.+?)public', ticket)\n",
    "                    #Remove escaped newline '\\\\n' and non-breaking space 'nbsp' characters\n",
    "                    m = [re.sub(r'\\\\n|nbsp', ' ', t) for t in l]\n",
    "                    # Remove any URL within a string\n",
    "                    p = [re.sub(r'http\\S+|www\\S+', '', o) for o in m]          \n",
    "                    # Remove all of the punctuation in any item in the list. The result is for each ticket a list of comments.\n",
    "                    q = [''.join(c for c in s if c not in punctuation) for s in p]\n",
    "                    # Join list elements without any separator. The result is for each ticket a list of merged comments.\n",
    "                    r = [' '.join(q)] \n",
    "                    yield(r)\n",
    "                \n",
    "    if __name__ == '__main__':\n",
    "        extract_ticket_text_generator(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt_files():\n",
    "        \"\"\"This function takes a list of text strings and saves each ticket in a .txt file.\"\"\"\n",
    "        data = extract_ticket_text_generator(filename)\n",
    "        # Make a flat list out of list of lists.\n",
    "        flat_list = [item for sublist in data for item in sublist]\n",
    "        for i in range(len(flat_list)):\n",
    "            with open(\"ticket_%d.txt\" % (i+1), 'w', encoding=\"utf-8\") as f:\n",
    "                f.write(flat_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_txt_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Create a .txt file with all tickets in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tickets_all_txt_file():\n",
    "        \"\"\"This function takes a list of text strings and saves all tickets in a .txt file.\"\"\"\n",
    "        data = extract_ticket_text_generator(filename)\n",
    "        # Make a flat list out of list of lists.\n",
    "        flat_list = [item for sublist in data for item in sublist]\n",
    "        with open('ticket_all.txt', 'w', encoding=\"utf-8\") as filehandle:\n",
    "            #Save all elements of a list as a text file:\n",
    "            for listitem in flat_list:\n",
    "                filehandle.write('%s\\n\\n' % listitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tickets_all_txt_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "    \n",
    "all_txt_files =[]\n",
    "for file in Path(\"zendesk_txt\").rglob(\"*.txt\"):\n",
    "    all_txt_files.append(file.parent / file.name)\n",
    "    # counts the length of the list\n",
    "    n_files = len(all_txt_files)\n",
    "    print(n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for txt_file in all_txt_files:\n",
    "    with open(txt_file, encoding=\"utf-8\") as f:\n",
    "        txt_file_as_string = f.read()\n",
    "        all_docs.append(txt_file_as_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>WordCloud<a class=\"anchor\" id=\"third-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Load the packages:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in terminal or command prompt\n",
    "# python3 -m spacy download en\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "#Wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Tokenize and Clean-up using gensim’s simple_preprocess()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(all_docs))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Lemmatization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom list of English stopwords\n",
    "custom_stopwords = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords=custom_stopwords,\n",
    "                      background_color=\"white\",\n",
    "                      width = 4000,\n",
    "                      height = 2000,\n",
    "                      max_words=200, \n",
    "                      collocations = False,   #remove repetitive words\n",
    "                      min_word_length = 3\n",
    "                         ).generate(' '.join(data_lemmatized))\n",
    "    \n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "wordcloud.to_file(\"word_cloud.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>CountVectorizer<a class=\"anchor\" id=\"fourth-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Create the Document-Word matrix</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer=CountVectorizer(analyzer='word',   \n",
    "                           token_pattern='[a-zA-Z]{3,}', # only non-digit characters > 3\n",
    "                           stop_words=custom_stopwords,  # remove stop words\n",
    "                           lowercase=True,               # convert all words to lowercase\n",
    "                           # min_df=10,                  # minimum reqd occurences of a word\n",
    "                           # max_features=50000,         # max number of uniq words\n",
    "                           \n",
    "                       )\n",
    "    \n",
    "# this step generates word counts for the words in your docs \n",
    "data_vectorized=vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 rows (6 tickets), 54 columns (unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check rows(docs) and columns(unique words), minus single character words\n",
    "#The columns number is raw word frequency\n",
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times a word has been used in a ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Count</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top_n_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count column in Excel spreadsheet\n",
    "np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "sorted_words_freq =sorted(words_freq, \n",
    "                          key = lambda x: x[1], \n",
    "                          reverse=True)\n",
    "sorted_words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(sorted_words_freq[:200],\n",
    "                         columns=['words', 'count'])\n",
    "\n",
    "\n",
    "dataframe.head(201)\n",
    "\n",
    "dataframe.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows in pandas dataframe\n",
    "sliced = dataframe.iloc[[1,2,7,8,9,10,11,13,14,16,19,20,25,26,27,29,30,35,38,39,40,41,49,55,59,64,65,66,67,69,71,76,78,79,80,81,84,87,88,92,93,96,99,104,108,119,126,137,140,146,147,148,149,150,162,164,168,185,186,197], [0,1]]\n",
    "\n",
    "sliced.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create horizontal barplot\n",
    "ax = sliced.head(10).plot.barh(x='words', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification for count of specific word\n",
    "grep -o -i support *.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 393559. Check where is the discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "# Plot horizontal bar graph\n",
    "dataframe.head(10).sort_values(by='count').plot.barh(x='words',\n",
    "                                                     y='count',\n",
    "                                                     ax=ax,\n",
    "                                                     color=\"#a3e8e5\",\n",
    "                                                     fontsize=16,\n",
    "                                                     xlabel='Words')\n",
    "\n",
    "ax.set_title(\"Top ten words in Zendesk tickets\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Document Frequency</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification for document frequency of a specific word\n",
    "grep -i support *txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 34654. Check where is the discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count strings with substring via string list\n",
    "#Verification for document frequency of a specific word\n",
    "subs = 'support'\n",
    "res = len([i for i in all_docs if subs in i]) \n",
    "print (\"All strings count with given substring are : \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count in how many strings within a list a specific substring appears\n",
    "#Verification for document frequency of a specific word\n",
    "substring = \"support\"\n",
    "l = []\n",
    "for x in all_docs:\n",
    "    if substring in x:\n",
    "        l.append(1)\n",
    "    print(substring, len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>TfidfTransformer</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Smoothed-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "# Plot horizontal bar graph\\n\",\n",
    "dataframe.head(10).sort_values(by='count').plot.barh(x='words',\n",
    "                                                     y='count',\n",
    "                                                     ax=ax,\n",
    "                                                     color=\"#a3e8e5\",\n",
    "                                                     fontsize=16, \n",
    "                                                     xlabel='Words')\n",
    "    \n",
    "ax.set_title(\"Ten most frequent words in Zendesk tickets\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values:\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, \n",
    "                      index=cv.get_feature_names(),\n",
    "                      columns=[\"idf_weights\"]) \n",
    "    \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>TF_IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix\n",
    "count_vector=cv.transform(all_docs)\n",
    "\n",
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    "    \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    "    \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    " \n",
    "# just send in all your docs here\n",
    "fitted_vectorizer=tfidf_vectorizer.fit(all_docs)\n",
    "tfidf_vectorizer_vectors=fitted_vectorizer.transform(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Sparsicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LDA model with sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STUDY MORE WHAT IS THE BELOW ABOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                      )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in lda_model.get_topic_terms(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnose model performance with perplexity and log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-likelihood higher the better. Perplexity lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to GridSearch the best LDA model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to see the best topic model and its parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare LDA Model Performance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [10, 15, 20, 25, 30]\n",
    "\n",
    "log_likelyhoods_5 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.9]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to see the dominant topic in each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review topics distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to visualize the LDA model with pyLDAvis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable the automatic display of visualizations in the IPython Notebook.<br>\n",
    "Transform and prepare a LDA model’s data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis - Python library for interactive topic model visualization. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the visualization to a stand-alone HTML file for easy sharing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "p = pyLDAvis.gensim.prepare(best_lda_model, data_vectorized, vectorizer)\n",
    "pyLDAvis.save_html(p, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
