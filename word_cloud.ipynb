{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Zendesk Tickets Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Obtaining data from APIs](#first-bullet)\n",
    "\n",
    "* [Preprocessing](#second-bullet)\n",
    "\n",
    "* [WordCloud](#third-bullet)\n",
    "\n",
    "* [CountVectorizer](#fourth-bullet)\n",
    "\n",
    "* [LDA Model](#fifth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "WORKING"
    ]
   },
   "source": [
    "Set up the development environment: \n",
    "- Install Git\n",
    "- Install Anaconda distribution (bookmarks)\n",
    "\n",
    "- Install Node.js\n",
    "- Install Postman native app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "source": [
    "Create new Python environment and check if we are in the right environment:\n",
    "- create the new environment without asking for confirmation (conda create --name testenv pandas jupyter ipykernel tabulate -y)\n",
    "- activate the environment (conda activate testenv)\n",
    "- use ipykernel to register your new environment as a kernel named testenv (python -m ipykernel install --user --name testenv --display-name \"testenv\")\n",
    "- import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "WORKING"
    ]
   },
   "source": [
    "In the test environment install the following packages:\n",
    "- Install ijson (conda install)\n",
    "- Install natsort (conda install)\n",
    "- Install gensim (pip install)\n",
    "- Install spacy (conda install)\n",
    "- Download package en-core-web-sm (python -m spacy download en)- restart jupyter notebook\n",
    "- Install wordcloud (conda install -c conda-forge wordcloud)\n",
    "- Install nltk (conda install)\n",
    "\n",
    "- Install matplotlib (conda install)\n",
    "- Install package sklearn (pip install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test environment for jupyter unresponsive error install the following: \n",
    "- Install package nbstripout (pip install nbstripout)\n",
    "- To clear the error: in terminal nbstripout filename.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file diagrams/block_diagram\n",
    "blockdiag {\n",
    "    orientation = portrait\n",
    "\n",
    "  A -> B -> C -> D -> E -> F;\n",
    "\n",
    "  // Set labels to nodes.\n",
    "   A [label = \"Obtaining data from APIs\"];\n",
    "   B [label = \"Preprocessing\"];\n",
    "   C [label = \"WordCloud\"];\n",
    "   D [label = \"CountVectorizer\"];\n",
    "   E [label = \"LDA Model\"];\n",
    "   F [label = \"Test\"];\n",
    " \n",
    "  // Set boder-style, background-color and text-color to nodes.\n",
    "  \n",
    "   A,B,C,D,E,F [color = \"#31708f\", textcolor=\"#FFFFFF\"];\n",
    "\n",
    "  // Set width and height to nodes.\n",
    "   A,B,C,D,E,F [width = 300]; // default value is 128\n",
    "    \n",
    "  // Set fontsize\n",
    "  default_fontsize = 20;  // default value is 11\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!blockdiag diagrams/block_diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"diagrams/block_diagram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "WORKING"
    ]
   },
   "source": [
    "Set up jekyll for gh pages:\n",
    "- Install jekyll (bookmarks)\n",
    "\n",
    "- Install Ruby (verify installation: ruby -v; gem -v)\n",
    "- cd Documents (in git bash)\n",
    "- jekyll new mynewsite\n",
    "- cd mynewsite\n",
    "- bundle exec jekyll serve\n",
    "- gem \"jekyll-theme-hydeout\" (in the gemfile)\n",
    "- bundle install (in git bash)\n",
    "- theme: jekyll-theme-hydeout (in _config file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test environment for word cloud TF-IDF install the following:\n",
    "- Install package numpy (conda install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new environment for topic modelling install the following:\n",
    "- Install package pandas (conda install)\n",
    "\n",
    "\n",
    "\n",
    "- Install package pyLDAvis (pip install)\n",
    "- Install package scipy (conda install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test environment for block diagram install the following:\n",
    "- Install package blockdiag (pip install)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a package is installed:\n",
    "- In Anaconda Prompt (run as Admin)\n",
    "- conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Anaconda Prompt</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of all my environments, active environment is shown with *. It will display the base environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new environment named testenv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n testenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the new environment created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Jupyter Lab: http://localhost:8888/lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Obtaining data from APIs<a class=\"anchor\" id=\"first-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">In Git Bash</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Desktop/JUPYTER_NOTEBOOKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install Newman (command-line collection runner for Postman):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npm install newman -global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Base64</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUTHORIZATION - Basic Authentication and API tokens<br> \n",
    "Use your company email address and Zendesk API key. The credentials must be sent in an Authorization header in the HTTP request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate a request with basic authentication and API token:\n",
    "- Combine your email address/token with your Zendesk API key with a colon:\n",
    "```svetlana.staneva@eventsforce.com/token: {zendesk_api_key}```\n",
    "- Base64-encode the resulting string:\n",
    "```amRvZUBleGFtcGxlLmNvbTpwYSQkdzByZA==```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In TextMechanic<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate a lists of numbers (1-3000, 3001-5000, etc.) up to 59046\n",
    "- Create a runner.csv files with a column ticket_id and listing numbers from 1 to 3000, etc.\n",
    "- Move the files to the working directory JUPYTER_NOTEBOOKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Postman<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postman Comments Collection<br>\n",
    "The Comments is a Postman collection that lists comments for Zendesk tickets from number 1 to 59046."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new environment:\n",
    "- Click on the Cog icon\n",
    "- Click Add\n",
    "- In Add Environment enter a name for the environment - for instance, Test \n",
    "- Click Add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an active environment:\n",
    "- Click the dropdown menu in the upper right corner of the Postman app to select an active environment (Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request data via GET request: https://eventsforce.zendesk.com/api/v2/tickets/{ticket_id}/comments.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Headers include the base64-encoded string:\n",
    "- In Headers go to Presets > Manage Presets\n",
    "- Add the Authorization and click Add:\n",
    "```Authorization: Basic {base64-encoded-string}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tests add the following line of code:\n",
    "```tests[responseBody] = true;```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Comments Collection\n",
    "- Click on Collections > Create New Collection\n",
    "- Enter a name - for instance, Comments and click Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Request in the Collection:\n",
    "- Click on Save As\n",
    "- Select the Collection and click Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the collection locally in the Postman Collection Runner with the runner_head.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the Comments collection: \n",
    "- Click on Collections \n",
    "- Hover over the Comments collection and click on the dots '...' that appear\n",
    "- Click Export and again Export\n",
    "- Save the file on the Desktop\n",
    "- The exported file is COMMENTS.postman_collection.json\n",
    "- Move the file COMMENTS.postman_collection.json to the working directory JUPYTER_NOTEBOOKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Git Bash</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Comments collection with the additional runner.csv file of key values (by 3000) and generate report in json.<br>\n",
    "```newman run COMMENTS.postman_collection.json -d runner.csv -r cli,json```<br>\n",
    "In case it gives an error 'JavaScript heap out of memory' run the code in the following format:<br>\n",
    "```NODE_OPTIONS=--max_old_space_size=2048 newman run COMMENTS.postman_collection.json -d runner.csv -r cli,json```<br>\n",
    "or<br>\n",
    "```NODE_OPTIONS=--max_old_space_size=3072 newman run COMMENTS.postman_collection.json -d runner.csv -r cli,json```<br>\n",
    "or run the Comments collection with the additional runner.csv file of key values (by 1000) and generate report in json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run multiple runs with different runner.csv files, create a name.sh file listing the commands:<br>\n",
    "```newman run COMMENTS.postman_collection.json -d runner_1000.csv -r cli,json```<br>\n",
    "```newman run COMMENTS.postman_collection.json -d runner_2000.csv -r cli,json```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the .sh file:<br>\n",
    "```bash name.sh```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Jupyter Notebook</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the working environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">In Jupyter Notebook</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Preprocessing<a class=\"anchor\" id=\"second-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "pwd   #Display the path of current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "#Make sure glob.glob returns a list of files. List all json files in newman folder, sorted in ascending order.\n",
    "file_list = sorted(glob.glob('/home/smsta/Desktop/zendesk_tag_cloud/newman/*.json'))\n",
    "for filename in file_list:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "def parse_json(json_filename):\n",
    "        with open(filename, 'r', encoding=\"utf8\") as file:\n",
    "            # load json iteratively\n",
    "            parser = ijson.parse(file)\n",
    "            for prefix, event, value in parser:\n",
    "                print('prefix={}, event={}, value={}'.format(prefix, event, value))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parse_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "from string import punctuation\n",
    "def extract_ticket_text_generator(json_filename):\n",
    "    \"\"\"This function takes a list of files with tickets and extracts text from each ticket. The result is a list of text strings.\"\"\"\n",
    "    for filename in file_list:\n",
    "            # When you open the file specify the encoding.\n",
    "            with open(filename, 'r', encoding=\"utf8\") as input_file:\n",
    "                # Extract specific items from the file\n",
    "                tickets = ijson.items(input_file, 'run.executions.item.assertions.item.assertion')\n",
    "                for ticket in tickets:\n",
    "                   # Extract the substring between two markers\n",
    "                    l = re.findall('plain_body(.+?)public', ticket)\n",
    "                    #Remove escaped newline '\\\\n' and non-breaking space 'nbsp' characters\n",
    "                    m = [re.sub(r'\\\\n|nbsp', ' ', t) for t in l]\n",
    "                    # Remove any URL within a string\n",
    "                    p = [re.sub(r'http\\S+|www\\S+', '', o) for o in m]          \n",
    "                    # Remove all of the punctuation in any item in the list. The result is for each ticket a list of comments.\n",
    "                    q = [''.join(c for c in s if c not in punctuation) for s in p]\n",
    "                    # Join list elements without any separator. The result is for each ticket a list of merged comments.\n",
    "                    r = [' '.join(q)] \n",
    "                    yield(r)\n",
    "                \n",
    "    if __name__ == '__main__':\n",
    "        extract_ticket_text_generator(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "def create_txt_files():\n",
    "        \"\"\"This function takes a list of text strings and saves each ticket in a .txt file.\"\"\"\n",
    "        data = extract_ticket_text_generator(filename)\n",
    "        # Make a flat list out of list of lists.\n",
    "        flat_list = [item for sublist in data for item in sublist]\n",
    "        for i in range(len(flat_list)):\n",
    "            with open(\"ticket_%d.txt\" % (i+1), 'w', encoding=\"utf-8\") as f:\n",
    "                f.write(flat_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "create_txt_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Desktop/zendesk_tag_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Create a .txt file with all tickets in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tickets_all_txt_file():\n",
    "        \"\"\"This function takes a list of text strings and saves all tickets in a .txt file.\"\"\"\n",
    "        data = extract_ticket_text_generator(filename)\n",
    "        # Make a flat list out of list of lists.\n",
    "        flat_list = [item for sublist in data for item in sublist]\n",
    "        with open('ticket_all.txt', 'w', encoding=\"utf-8\") as filehandle:\n",
    "            #Save all elements of a list as a text file:\n",
    "            for listitem in flat_list:\n",
    "                filehandle.write('%s\\n\\n' % listitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tickets_all_txt_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In zendesk_tag_cloud create folder zendesk_txt and move all .txt files generated from create_txt_files() function into it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Unigram WordCloud<a class=\"anchor\" id=\"third-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_txt_files =[]\n",
    "#control order of results from iterator\n",
    "for file in sorted(Path(\"zendesk_txt\").iterdir()):\n",
    "    all_txt_files.append(file.name)\n",
    "print(all_txt_files[:6])\n",
    "    # counts the length of the list\n",
    "len(all_txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "import natsort \n",
    "\n",
    "#sort filenames in directory ascending\n",
    "all_txt_files_sorted = natsort.natsorted(all_txt_files)\n",
    "all_txt_files_sorted[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "cd zendesk_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for txt_file in all_txt_files_sorted:\n",
    "    with open(txt_file, encoding=\"utf-8\") as f:\n",
    "        txt_file_as_string = f.read()\n",
    "        all_docs.append(txt_file_as_string)\n",
    "all_docs[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Load the packages:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in terminal or command prompt\n",
    "# python3 -m spacy download en\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy\n",
    "\n",
    "#Wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Tokenize and Clean-up using gensimâ€™s simple_preprocess()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(all_docs))\n",
    "\n",
    "print(data_words[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Lemmatization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS \n",
    "\n",
    "#Create custom list of English stopwords, signature words listed after \"wonder\"\n",
    "custom_stopwords = list(STOPWORDS)+[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS \n",
    "\n",
    "#Create custom list of English stopwords, signature words listed after \"wonder\"\n",
    "custom_stopwords = list(STOPWORDS)+[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\", \"regard\", \"kind\", \"technical\", \"ticket\", \"message\", \"team\", \"click\", \"day\", \"work\", \"close\", \"issue\", \"check\", \"pivotal\", \"confidential\", \"tracker\", \"attachment\", \"copy\", \"time\", \"follow\", \"free\", \"round\", \"select\", \"good\", \"detail\", \"recipient\", \"virus\", \"datum\", \"notify\", \"intend\", \"sender\", \"delegate\", \"story\", \"leader\", \"address\", \"assistance\", \"feel\", \"accept\", \"step\", \"option\", \"find\", \"reply\", \"call\", \"item\", \"reopen\", \"provide\", \"office\", \"respond\", \"note\", \"happen\", \"setup\", \"lovely\", \"save\", \"confirm\", \"advise\", \"disclose\", \"addressee\", \"hope\", \"development\", \"business\", \"privileged\", \"comment\", \"great\", \"entry\", \"solely\", \"display\", \"answer\", \"company\", \"intended\", \"expect\", \"include\", \"inquiry\", \"problem\", \"start\", \"case\", \"remove\", \"open\", \"base\", \"card\", \"icon\", \"subject\", \"response\", \"hear\", \"liability\", \"head\", \"chat\", \"text\", \"correct\", \"morning\", \"button\", \"write\", \"reach\", \"medium\", \"alternative\", \"share\", \"limit\", \"pull\", \"url\", \"malware\", \"damage\", \"purpose\", \"touch\", \"screenshot\", \"audit\", \"article\", \"colleague\", \"notice\", \"action\", \"bookable\", \"customer\", \"investigate\", \"tutorial\", \"finance\", \"attach\", \"understand\", \"store\", \"query\", \"loss\", \"speak\", \"external\", \"result\", \"responsibility\", \"forward\", \"trail\", \"discuss\", \"solution\", \"place\", \"state\", \"apply\", \"experience\", \"mention\", \"reason\", \"raise\", \"additional\", \"today\", \"visit\", \"conference\", \"submit\", \"outline\", \"distribute\", \"direct\", \"print\", \"legally\", \"screen\", \"bug\", \"monitor\", \"fine\", \"tttttttt\", \"read\", \"minute\", \"management\", \"full\", \"year\", \"operator\", \"replicate\", \"transaction\", \"administrator\", \"individual\", \"recommend\", \"backend\", \"method\", \"twitter\", \"point\", \"agenda\", \"database\", \"transmit\", \"exist\", \"afraid\", \"carry\", \"virtual\", \"proprietary\", \"prohibit\", \"dependent\", \"confirmation\", \"resolve\", \"connect\", \"exam\", \"quick\", \"correctly\", \"unable\", \"student\", \"marketing\", \"mark\", \"credit\", \"security\", \"garry\", \"deliver\", \"notification\", \"british\", \"assist\", \"easy\", \"ill\", \"retain\", \"relation\", \"original\", \"helpful\", \"testing\", \"future\", \"platform\", \"browser\", \"amp\", \"thing\", \"week\", \"attempt\", \"long\", \"network\", \"amendment\", \"active\", \"specific\", \"opportunity\", \"staff\", \"happy\", \"moment\", \"strictly\", \"apology\", \"instance\", \"hour\", \"form\", \"sort\", \"cultural\", \"educational\", \"previous\", \"requirement\", \"authorise\", \"continue\", \"return\", \"bottom\", \"confidentiality\", \"council\", \"dropdown\", \"leave\", \"environment\", \"tomorrow\", \"phone\", \"clear\", \"lot\", \"fail\", \"bot\", \"online\", \"charge\", \"muir\", \"gayle\", \"box\", \"coordinator\", \"vinay\", \"proceed\", \"behalf\", \"separate\", \"replication\", \"description\", \"tool\", \"menu\", \"visible\", \"early\", \"info\", \"safe\", \"software\", \"mandatory\", \"certificate\", \"developer\", \"calendar\", \"move\", \"schroder\", \"design\", \"entire\", \"manually\", \"current\", \"function\", \"caution\", \"general\", \"checkbox\", \"opinion\", \"blank\", \"reserve\", \"delivery\", \"refund\", \"society\", \"automatically\", \"talk\", \"supplementary\", \"explain\", \"clarify\", \"idea\", \"relate\", \"blog\", \"disclosure\", \"selection\", \"turn \", \"default\", \"represent\", \"nice\", \"cancellation\", \"block\", \"policy\", \"college\", \"shortly\", \"copying\", \"bit\", \"practice\", \"senior\", \"wait\", \"submitter\", \"directly\", \"lawful\", \"claire\", \"computer\", \"purchase\", \"wenlock\", \"guarantee\", \"period\", \"market\", \"express\", \"profile\", \"usual\", \"warrant\", \"disclaimer\", \"aware\", \"deactivate\", \"wrong\", \"reliance\", \"product\", \"sandbox\", \"switch\", \"subsidiary\", \"month\", \"weekend\", \"fee\", \"country\", \"sophie\", \"fire\", \"charity\", \"statement\", \"virusfree\", \"registered\", \"functionality\", \"common\", \"tarla\", \"tttt\", \"currency\", \"choose\", \"floor\", \"title\", \"provider\", \"government\", \"private\", \"hesitate\", \"adjust\", \"saml\", \"outstanding\", \"pencil\", \"svetlana\", \"panel\", \"delay\", \"attention\", \"regulation\", \"chemistry\", \"instruction\", \"member\", \"scenario\", \"dev\", \"originate\", \"advice\", \"afternoon\", \"release\", \"side\", \"road\", \"equal\", \"trigger\", \"technology\", \"wale\", \"pick\", \"webinar\", \"royal\", \"affect\", \"merge\", \"format\", \"term\", \"guide\", \"peaceful\", \"redirect\", \"solve\", \"money\", \"hold\", \"transmission\", \"invite\", \"emma\", \"distribution\", \"incomplete\", \"chrome\", \"organisation\", \"property\", \"working\", \"cost\", \"emily\", \"expire\", \"school\", \"sense\", \"tick\", \"preview\", \"assume\", \"copyright\", \"glad\", \"special\", \"news\", \"asap\", \"material\", \"join\", \"prevent\", \"application\", \"electronic\", \"ahead\", \"large\", \"receipt\", \"history\", \"feedback\", \"restrict\", \"trust\", \"post\", \"double\", \"highlight\", \"relevant\", \"successful\", \"frontend\", \"area\", \"cookie\", \"evening\", \"simply\", \"enable\", \"yesterday\", \"sound\", \"cheer\", \"valente\", \"archive\", \"winner\", \"telephone\", \"invitee\", \"reminder\", \"failure\", \"transfer\", \"host\", \"staging\", \"anshu\", \"situation\", \"offer\", \"couple\", \"contract\", \"tatiana\", \"booker\", \"pricing\", \"eileen\", \"unauthorised\", \"faculty\", \"art\", \"legal\", \"global\", \"corner\", \"exclude\", \"destroy\", \"employee\", \"green\", \"standard\", \"basis\", \"ellie\", \"consequence\", \"workaround\", \"character\", \"entity\", \"law\", \"officer\", \"centre\", \"engage\", \"extract\", \"chance\", \"regulate\", \"bulk\", \"cover\", \"endeavour\", \"tech\", \"adrian\", \"finish\", \"annual\", \"decide\", \"condition\", \"exceptional\", \"sabine\", \"poster\", \"quantity\", \"inform\", \"invalid\", \"prompt\", \"remember\", \"executive\", \"unlawful\", \"hover\", \"maguire\", \"firstname\", \"lock\", \"privilege\", \"markedasprinte\", \"eventforce\", \"tania\", \"incorrect\", \"haymarket\", \"mind\", \"header\", \"lastname\", \"organiser\", \"locate\", \"ielt\", \"melissa\", \"internal\", \"switchboard\", \"adhoc\", \"risk\", \"planner\", \"determine\", \"candidate\", \"normal\", \"origin\", \"unpaid\", \"anthony\", \"avoid \", \"operational\", \"venue\", \"knowledge\", \"identify\", \"row\", \"handle\", \"extend\", \"fre\", \"footer\", \"band\", \"sharma\", \"jpg\", \"party\", \"intercept\", \"valid\", \"ebook\", \"sector\", \"worry\", \"occur\", \"TRUE\", \"professional\", \"investigation\", \"actuary\", \"internet\", \"connection\", \"regulatory\", \"widget\", \"reasonable\", \"achieve\", \"typically\", \"properly\", \"encounter\", \"scroll\", \"small\", \"exact\", \"dynamic\", \"success\", \"spam\", \"recognise\", \"exclusive\", \"lose\", \"window\", \"convert\", \"remotely\", \"crash\", \"unsubmitted\", \"straight\", \"dear\", \"expert\", \"impact\", \"investment\", \"precaution\", \"iop\", \"remain\", \"collect\", \"matter\", \"mode\", \"clarification\", \"level\", \"filipa\", \"trevor\", \"fisher\", \"manager\", \"support\", \"apac\", \"web\", \"eventsforce\", \"harry\", \"layla\", \"sabharwal\", \"centaur\", \"charlotte\", \"kaushik\", \"ella\", \"lyndsey\", \"tanya\", \"argus\", \"vuong\", \"director\", \"nguyen\", \"diane\", \"howard\", \"nacp\", \"kirstie\", \"burton\", \"aoc\", \"tttttt\", \"webthere\", \"masterclasse\", \"uptodate\", \"breakfast\", \"oso\", \"osoble\", \"marcelo\", \"ticketassigne\", \"quickmobile\", \"late\", \"view\", \"interested\", \"exclusively\", \"shoot\", \"fiona\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS \n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "#stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords = custom_stopwords,\n",
    "                      background_color=\"white\",\n",
    "                      width = 4000,\n",
    "                      height = 2000,\n",
    "                      max_words=200, \n",
    "                      collocations = False,   #remove repetitive words\n",
    "                      min_word_length = 3, \n",
    "                      random_state = 23        #get the same word cloud \n",
    "                         ).generate(''.join(data_lemmatized))\n",
    "    \n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "wordcloud.to_file(\"v1_word_cloud_unigrams.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Dataframe from Unigram WordCloud<a class=\"anchor\" id=\"third-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "# generate word count from WordCloud\n",
    "word_list = WordCloud(stopwords = custom_stopwords,\n",
    "                      max_words=200, \n",
    "                      collocations = False,   #remove repetitive words\n",
    "                      min_word_length = 3, ).process_text(''.join(data_lemmatized))\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "# sort dictionary by value\n",
    "sorted_words = sorted(word_list.items(), \n",
    "                      key=lambda x: x[1], \n",
    "                      reverse=True)\n",
    "sorted_words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(sorted_words,\n",
    "                         columns=['words', 'count'])\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Top 30 Words in Zendesk Tickets<a class=\"anchor\" id=\"third-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "# plot horizontal bar graph\n",
    "ax = dataframe.head(30).sort_values(by='count').plot(kind='barh', \n",
    "                                                     figsize=(20,12), \n",
    "                                                     x='words', \n",
    "                                                     y='count', \n",
    "                                                     color=\"#ccbaba\",\n",
    "                                                     fontsize=16, \n",
    "                                                     xlabel=\"\", \n",
    "                                                     width=0.75)\n",
    "#annotate pandas plot bars\n",
    "for patch in ax.patches:\n",
    "    ax.text(patch.get_width() + 0.3, \n",
    "            patch.get_y() + 0.2,\n",
    "            \" {:,}\".format(patch.get_width()), \n",
    "            fontsize=12, \n",
    "            fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change figure background color and save\n",
    "fig = ax.get_figure()\n",
    "fig.patch.set_facecolor('#FFFFFF')\n",
    "fig.savefig(\"v1_top_30_unigram.png\", facecolor=fig.get_facecolor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Bigram WordCloud<a class=\"anchor\" id=\"third-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords = custom_stopwords,\n",
    "                      background_color=\"white\",\n",
    "                      width = 4000,\n",
    "                      height = 2000,\n",
    "                      max_words=200, \n",
    "                      min_word_length = 3, \n",
    "                      collocation_threshold = 3,   #create wordcloud of bigrams\n",
    "                      random_state = 23        #get the same word cloud\n",
    "                         ).generate(''.join(data_lemmatized))\n",
    "\n",
    "    \n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "wordcloud.to_file(\"lockdown_word_cloud_bigram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Dataframe from Bigram WordCloud<a class=\"anchor\" id=\"third-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ''.join(data_lemmatized)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find n-grams\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "#remove stopwords from string\n",
    "words_list = result.split()\n",
    "custom_stopwords = list(STOPWORDS)+[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\", \"regard\", \"kind\", \"technical\", \"ticket\", \"message\", \"team\", \"click\", \"day\", \"work\", \"close\", \"issue\", \"check\", \"pivotal\", \"confidential\", \"tracker\", \"attachment\", \"copy\", \"time\", \"follow\", \"free\", \"round\", \"select\", \"good\", \"detail\", \"recipient\", \"virus\", \"datum\", \"notify\", \"intend\", \"sender\", \"delegate\", \"story\", \"leader\", \"address\", \"assistance\", \"feel\", \"accept\", \"step\", \"option\", \"find\", \"reply\", \"call\", \"item\", \"reopen\", \"provide\", \"office\", \"respond\", \"note\", \"happen\", \"setup\", \"lovely\", \"save\", \"confirm\", \"advise\", \"disclose\", \"addressee\", \"hope\", \"development\", \"business\", \"privileged\", \"comment\", \"great\", \"entry\", \"solely\", \"display\", \"answer\", \"company\", \"intended\", \"expect\", \"include\", \"inquiry\", \"problem\", \"start\", \"case\", \"remove\", \"open\", \"base\", \"card\", \"icon\", \"subject\", \"response\", \"hear\", \"liability\", \"head\", \"chat\", \"text\", \"correct\", \"morning\", \"button\", \"write\", \"reach\", \"medium\", \"alternative\", \"share\", \"limit\", \"pull\", \"url\", \"malware\", \"damage\", \"purpose\", \"touch\", \"screenshot\", \"audit\", \"article\", \"colleague\", \"notice\", \"action\", \"bookable\", \"customer\", \"investigate\", \"tutorial\", \"finance\", \"attach\", \"understand\", \"store\", \"query\", \"loss\", \"speak\", \"external\", \"result\", \"responsibility\", \"forward\", \"trail\", \"discuss\", \"solution\", \"place\", \"state\", \"apply\", \"experience\", \"mention\", \"reason\", \"raise\", \"additional\", \"today\", \"visit\", \"conference\", \"submit\", \"outline\", \"distribute\", \"direct\", \"print\", \"legally\", \"screen\", \"bug\", \"monitor\", \"fine\", \"tttttttt\", \"read\", \"minute\", \"management\", \"full\", \"year\", \"operator\", \"replicate\", \"transaction\", \"administrator\", \"individual\", \"recommend\", \"backend\", \"method\", \"twitter\", \"point\", \"agenda\", \"database\", \"transmit\", \"exist\", \"afraid\", \"carry\", \"virtual\", \"proprietary\", \"prohibit\", \"dependent\", \"confirmation\", \"resolve\", \"connect\", \"exam\", \"quick\", \"correctly\", \"unable\", \"student\", \"marketing\", \"mark\", \"credit\", \"security\", \"garry\", \"deliver\", \"notification\", \"british\", \"assist\", \"easy\", \"ill\", \"retain\", \"relation\", \"original\", \"helpful\", \"testing\", \"future\", \"platform\", \"browser\", \"amp\", \"thing\", \"week\", \"attempt\", \"long\", \"network\", \"amendment\", \"active\", \"specific\", \"opportunity\", \"staff\", \"happy\", \"moment\", \"strictly\", \"apology\", \"instance\", \"hour\", \"form\", \"sort\", \"cultural\", \"educational\", \"previous\", \"requirement\", \"authorise\", \"continue\", \"return\", \"bottom\", \"confidentiality\", \"council\", \"dropdown\", \"leave\", \"environment\", \"tomorrow\", \"phone\", \"clear\", \"lot\", \"fail\", \"bot\", \"online\", \"charge\", \"muir\", \"gayle\", \"box\", \"coordinator\", \"vinay\", \"proceed\", \"behalf\", \"separate\", \"replication\", \"description\", \"tool\", \"menu\", \"visible\", \"early\", \"info\", \"safe\", \"software\", \"mandatory\", \"certificate\", \"developer\", \"calendar\", \"move\", \"schroder\", \"design\", \"entire\", \"manually\", \"current\", \"function\", \"caution\", \"general\", \"checkbox\", \"opinion\", \"blank\", \"reserve\", \"delivery\", \"refund\", \"society\", \"automatically\", \"talk\", \"supplementary\", \"explain\", \"clarify\", \"idea\", \"relate\", \"blog\", \"disclosure\", \"selection\", \"turn \", \"default\", \"represent\", \"nice\", \"cancellation\", \"block\", \"policy\", \"college\", \"shortly\", \"copying\", \"bit\", \"practice\", \"senior\", \"wait\", \"submitter\", \"directly\", \"lawful\", \"claire\", \"computer\", \"purchase\", \"wenlock\", \"guarantee\", \"period\", \"market\", \"express\", \"profile\", \"usual\", \"warrant\", \"disclaimer\", \"aware\", \"deactivate\", \"wrong\", \"reliance\", \"product\", \"sandbox\", \"switch\", \"subsidiary\", \"month\", \"weekend\", \"fee\", \"country\", \"sophie\", \"fire\", \"charity\", \"statement\", \"virusfree\", \"registered\", \"functionality\", \"common\", \"tarla\", \"tttt\", \"currency\", \"choose\", \"floor\", \"title\", \"provider\", \"government\", \"private\", \"hesitate\", \"adjust\", \"saml\", \"outstanding\", \"pencil\", \"svetlana\", \"panel\", \"delay\", \"attention\", \"regulation\", \"chemistry\", \"instruction\", \"member\", \"scenario\", \"dev\", \"originate\", \"advice\", \"afternoon\", \"release\", \"side\", \"road\", \"equal\", \"trigger\", \"technology\", \"wale\", \"pick\", \"webinar\", \"royal\", \"affect\", \"merge\", \"format\", \"term\", \"guide\", \"peaceful\", \"redirect\", \"solve\", \"money\", \"hold\", \"transmission\", \"invite\", \"emma\", \"distribution\", \"incomplete\", \"chrome\", \"organisation\", \"property\", \"working\", \"cost\", \"emily\", \"expire\", \"school\", \"sense\", \"tick\", \"preview\", \"assume\", \"copyright\", \"glad\", \"special\", \"news\", \"asap\", \"material\", \"join\", \"prevent\", \"application\", \"electronic\", \"ahead\", \"large\", \"receipt\", \"history\", \"feedback\", \"restrict\", \"trust\", \"post\", \"double\", \"highlight\", \"relevant\", \"successful\", \"frontend\", \"area\", \"cookie\", \"evening\", \"simply\", \"enable\", \"yesterday\", \"sound\", \"cheer\", \"valente\", \"archive\", \"winner\", \"telephone\", \"invitee\", \"reminder\", \"failure\", \"transfer\", \"host\", \"staging\", \"anshu\", \"situation\", \"offer\", \"couple\", \"contract\", \"tatiana\", \"booker\", \"pricing\", \"eileen\", \"unauthorised\", \"faculty\", \"art\", \"legal\", \"global\", \"corner\", \"exclude\", \"destroy\", \"employee\", \"green\", \"standard\", \"basis\", \"ellie\", \"consequence\", \"workaround\", \"character\", \"entity\", \"law\", \"officer\", \"centre\", \"engage\", \"extract\", \"chance\", \"regulate\", \"bulk\", \"cover\", \"endeavour\", \"tech\", \"adrian\", \"finish\", \"annual\", \"decide\", \"condition\", \"exceptional\", \"sabine\", \"poster\", \"quantity\", \"inform\", \"invalid\", \"prompt\", \"remember\", \"executive\", \"unlawful\", \"hover\", \"maguire\", \"firstname\", \"lock\", \"privilege\", \"markedasprinte\", \"eventforce\", \"tania\", \"incorrect\", \"haymarket\", \"mind\", \"header\", \"lastname\", \"organiser\", \"locate\", \"ielt\", \"melissa\", \"internal\", \"switchboard\", \"adhoc\", \"risk\", \"planner\", \"determine\", \"candidate\", \"normal\", \"origin\", \"unpaid\", \"anthony\", \"avoid \", \"operational\", \"venue\", \"knowledge\", \"identify\", \"row\", \"handle\", \"extend\", \"fre\", \"footer\", \"band\", \"sharma\", \"jpg\", \"party\", \"intercept\", \"valid\", \"ebook\", \"sector\", \"worry\", \"occur\", \"TRUE\", \"professional\", \"investigation\", \"actuary\", \"internet\", \"connection\", \"regulatory\", \"widget\", \"reasonable\", \"achieve\", \"typically\", \"properly\", \"encounter\", \"scroll\", \"small\", \"exact\", \"dynamic\", \"success\", \"spam\", \"recognise\", \"exclusive\", \"lose\", \"window\", \"convert\", \"remotely\", \"crash\", \"unsubmitted\", \"straight\", \"dear\", \"expert\", \"impact\", \"investment\", \"precaution\", \"iop\", \"remain\", \"collect\", \"matter\", \"mode\", \"clarification\", \"level\", \"filipa\", \"trevor\", \"fisher\", \"manager\", \"support\", \"apac\", \"web\", \"eventsforce\", \"harry\", \"layla\", \"sabharwal\", \"centaur\", \"charlotte\", \"kaushik\", \"ella\", \"lyndsey\", \"tanya\", \"argus\", \"vuong\", \"director\", \"nguyen\", \"diane\", \"howard\", \"nacp\", \"kirstie\", \"burton\", \"aoc\", \"tttttt\", \"webthere\", \"masterclasse\", \"uptodate\", \"breakfast\", \"oso\", \"osoble\", \"marcelo\", \"ticketassigne\", \"quickmobile\", \"late\", \"view\", \"interested\", \"exclusively\", \"shoot\", \"fiona\"]\n",
    "words_without_stopwords = [word for word in words_list if word not in custom_stopwords]\n",
    "\n",
    "ngram_counts = Counter(ngrams(words_without_stopwords,2)) \n",
    "ngram_counts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(ngram_counts, orient='index').reset_index()\n",
    "df.columns =['words', 'count']\n",
    "\n",
    "#sort pandas dataframe\n",
    "df.sort_values(by=['count'], inplace=True, ascending=False)\n",
    "#join string\n",
    "df[\"words\"]= df[\"words\"].str.join(\"_\")\n",
    "df[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert pandas dataframe to list\n",
    "bigram_list_all = df.words.tolist()\n",
    "bigram_list = bigram_list_all[:200]\n",
    "bigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "NEXT"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords = custom_stopwords,\n",
    "                      background_color=\"white\",\n",
    "                      width = 4000,\n",
    "                      height = 2000,\n",
    "                      max_words=200, \n",
    "                      min_word_length = 3, \n",
    "                      collocation_threshold = 3,   #create wordcloud of bigrams\n",
    "                      random_state = 23        #get the same word cloud\n",
    "                         ).generate(''.join(bigram_list))\n",
    "\n",
    "    \n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Top 30 Bigrams in Zendesk Tickets<a class=\"anchor\" id=\"third-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot horizontal bar graph\n",
    "ax = df.head(30).sort_values(by='count').plot(kind='barh', \n",
    "                                                     figsize=(20,12), \n",
    "                                                     x='words', \n",
    "                                                     y='count', \n",
    "                                                     color=\"#ccbaba\",\n",
    "                                                     fontsize=16, \n",
    "                                                     xlabel=\"\", \n",
    "                                                     width=0.75)\n",
    "#annotate pandas plot bars\n",
    "for patch in ax.patches:\n",
    "    ax.text(patch.get_width() + 0.3, \n",
    "            patch.get_y() + 0.2,\n",
    "            \" {:,}\".format(patch.get_width()), \n",
    "            fontsize=12, \n",
    "            fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change figure background color and save\n",
    "fig = ax.get_figure()\n",
    "fig.patch.set_facecolor('#FFFFFF')\n",
    "fig.savefig(\"lockdown_top_30_bigram.png\", facecolor=fig.get_facecolor())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
