{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584f5cd1-7d9f-4ada-a01a-0e2f532794f5",
   "metadata": {},
   "source": [
    "In Terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0fdc2-153a-4e01-902a-823b1a17de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate testenv\n",
    "cd Desktop/zendesk_tag_cloud\n",
    "jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434e142-f0e9-48c3-9b49-fcb9674a7a30",
   "metadata": {},
   "source": [
    "List conda environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3a63b1-624d-4a83-bad7-2dfaa1b3b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home/smsta/anaconda3\n",
      "testenv               *  /home/smsta/anaconda3/envs/testenv\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa1b04-95cb-470d-9a76-b4c129e14ae6",
   "metadata": {},
   "source": [
    "Check installed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad14ccb-1c95-4805-874f-58d6c1568352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/smsta/anaconda3/envs/testenv:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main  \n",
      "_openmp_mutex             5.1                       1_gnu  \n",
      "anyio                     4.6.2           py312h06a4308_0  \n",
      "argon2-cffi               21.3.0             pyhd3eb1b0_0  \n",
      "argon2-cffi-bindings      21.2.0          py312h5eee18b_0  \n",
      "arrow                     1.3.0                    pypi_0    pypi\n",
      "asttokens                 2.0.5              pyhd3eb1b0_0  \n",
      "async-lru                 2.0.4           py312h06a4308_0  \n",
      "attrs                     24.2.0          py312h06a4308_0  \n",
      "babel                     2.11.0          py312h06a4308_0  \n",
      "beautifulsoup4            4.12.3          py312h06a4308_0  \n",
      "blas                      1.0                         mkl  \n",
      "bleach                    4.1.0              pyhd3eb1b0_0  \n",
      "bottleneck                1.3.7           py312ha883a20_0  \n",
      "brotli-python             1.0.9           py312h6a678d5_8  \n",
      "bzip2                     1.0.8                h5eee18b_6  \n",
      "ca-certificates           2024.9.24            h06a4308_0  \n",
      "certifi                   2024.8.30       py312h06a4308_0  \n",
      "cffi                      1.17.1          py312h1fdaa30_0  \n",
      "charset-normalizer        3.3.2              pyhd3eb1b0_0  \n",
      "comm                      0.2.1           py312h06a4308_0  \n",
      "cyrus-sasl                2.1.28               h52b45da_1  \n",
      "dbus                      1.13.18              hb2f20db_0  \n",
      "debugpy                   1.6.7           py312h6a678d5_0  \n",
      "decorator                 5.1.1              pyhd3eb1b0_0  \n",
      "defusedxml                0.7.1              pyhd3eb1b0_0  \n",
      "executing                 0.8.3              pyhd3eb1b0_0  \n",
      "expat                     2.6.3                h6a678d5_0  \n",
      "fontconfig                2.14.1               h55d465d_3  \n",
      "fqdn                      1.5.1                    pypi_0    pypi\n",
      "freetype                  2.12.1               h4a9f257_0  \n",
      "glib                      2.78.4               h6a678d5_0  \n",
      "glib-tools                2.78.4               h6a678d5_0  \n",
      "gst-plugins-base          1.14.1               h6a678d5_1  \n",
      "gstreamer                 1.14.1               h5eee18b_1  \n",
      "h11                       0.14.0          py312h06a4308_0  \n",
      "httpcore                  1.0.2           py312h06a4308_0  \n",
      "httpx                     0.27.0          py312h06a4308_0  \n",
      "icu                       73.1                 h6a678d5_0  \n",
      "idna                      3.7             py312h06a4308_0  \n",
      "intel-openmp              2023.1.0         hdb19cb5_46306  \n",
      "ipykernel                 6.29.5          py312h06a4308_0  \n",
      "ipython                   8.27.0          py312h06a4308_0  \n",
      "ipywidgets                8.1.2           py312h06a4308_0  \n",
      "isoduration               20.11.0                  pypi_0    pypi\n",
      "jedi                      0.19.1          py312h06a4308_0  \n",
      "jinja2                    3.1.4           py312h06a4308_0  \n",
      "joblib                    1.4.2           py312h06a4308_0  \n",
      "jpeg                      9e                   h5eee18b_3  \n",
      "json5                     0.9.6              pyhd3eb1b0_0  \n",
      "jsonpointer               3.0.0                    pypi_0    pypi\n",
      "jsonschema                4.23.0          py312h06a4308_0  \n",
      "jsonschema-specifications 2023.7.1        py312h06a4308_0  \n",
      "jupyter                   1.0.0           py312h06a4308_9  \n",
      "jupyter-lsp               2.2.0           py312h06a4308_0  \n",
      "jupyter_client            8.6.0           py312h06a4308_0  \n",
      "jupyter_console           6.6.3           py312h06a4308_1  \n",
      "jupyter_core              5.7.2           py312h06a4308_0  \n",
      "jupyter_events            0.10.0          py312h06a4308_0  \n",
      "jupyter_server            2.14.1          py312h06a4308_0  \n",
      "jupyter_server_terminals  0.4.4           py312h06a4308_1  \n",
      "jupyterlab                4.2.5           py312h06a4308_0  \n",
      "jupyterlab_pygments       0.1.2                      py_0  \n",
      "jupyterlab_server         2.27.3          py312h06a4308_0  \n",
      "jupyterlab_widgets        3.0.10          py312h06a4308_0  \n",
      "krb5                      1.20.1               h143b758_1  \n",
      "ld_impl_linux-64          2.40                 h12ee557_0  \n",
      "libclang                  14.0.6          default_hc6dbbc7_1  \n",
      "libclang13                14.0.6          default_he11475f_1  \n",
      "libcups                   2.4.2                h2d74bed_1  \n",
      "libedit                   3.1.20230828         h5eee18b_0  \n",
      "libffi                    3.4.4                h6a678d5_1  \n",
      "libgcc-ng                 11.2.0               h1234567_1  \n",
      "libgfortran-ng            11.2.0               h00389a5_1  \n",
      "libgfortran5              11.2.0               h1234567_1  \n",
      "libglib                   2.78.4               hdc74915_0  \n",
      "libgomp                   11.2.0               h1234567_1  \n",
      "libiconv                  1.16                 h5eee18b_3  \n",
      "libllvm14                 14.0.6               hecde1de_4  \n",
      "libpng                    1.6.39               h5eee18b_0  \n",
      "libpq                     12.17                hdbd6064_0  \n",
      "libsodium                 1.0.18               h7b6447c_0  \n",
      "libstdcxx-ng              11.2.0               h1234567_1  \n",
      "libuuid                   1.41.5               h5eee18b_0  \n",
      "libxcb                    1.15                 h7f8727e_0  \n",
      "libxkbcommon              1.0.1                h097e994_2  \n",
      "libxml2                   2.13.1               hfdd30dd_2  \n",
      "lz4-c                     1.9.4                h6a678d5_1  \n",
      "markupsafe                2.1.3           py312h5eee18b_0  \n",
      "matplotlib-inline         0.1.6           py312h06a4308_0  \n",
      "mistune                   2.0.4           py312h06a4308_0  \n",
      "mkl                       2023.1.0         h213fc3f_46344  \n",
      "mkl-service               2.4.0           py312h5eee18b_1  \n",
      "mkl_fft                   1.3.10          py312h5eee18b_0  \n",
      "mkl_random                1.2.7           py312h526ad5a_0  \n",
      "mysql                     5.7.24               h721c034_2  \n",
      "nbclient                  0.8.0           py312h06a4308_0  \n",
      "nbconvert                 7.16.4          py312h06a4308_0  \n",
      "nbformat                  5.10.4          py312h06a4308_0  \n",
      "ncurses                   6.4                  h6a678d5_0  \n",
      "nest-asyncio              1.6.0           py312h06a4308_0  \n",
      "notebook                  7.2.2           py312h06a4308_1  \n",
      "notebook-shim             0.2.3           py312h06a4308_0  \n",
      "numexpr                   2.10.1          py312h3c60e43_0  \n",
      "numpy                     1.26.4          py312hc5e2394_0  \n",
      "numpy-base                1.26.4          py312h0da6c21_0  \n",
      "openssl                   3.0.15               h5eee18b_0  \n",
      "overrides                 7.4.0           py312h06a4308_0  \n",
      "packaging                 24.1            py312h06a4308_0  \n",
      "pandas                    2.2.2           py312h526ad5a_0  \n",
      "pandocfilters             1.5.0              pyhd3eb1b0_0  \n",
      "parso                     0.8.3              pyhd3eb1b0_0  \n",
      "pcre2                     10.42                hebb0a14_1  \n",
      "pexpect                   4.8.0              pyhd3eb1b0_3  \n",
      "pip                       24.2            py312h06a4308_0  \n",
      "platformdirs              3.10.0          py312h06a4308_0  \n",
      "ply                       3.11            py312h06a4308_1  \n",
      "prometheus_client         0.14.1          py312h06a4308_0  \n",
      "prompt-toolkit            3.0.43          py312h06a4308_0  \n",
      "prompt_toolkit            3.0.43               hd3eb1b0_0  \n",
      "psutil                    5.9.0           py312h5eee18b_0  \n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2  \n",
      "pure_eval                 0.2.2              pyhd3eb1b0_0  \n",
      "pybind11-abi              5                    hd3eb1b0_0  \n",
      "pycparser                 2.21               pyhd3eb1b0_0  \n",
      "pygments                  2.15.1          py312h06a4308_1  \n",
      "pyqt                      5.15.10         py312h6a678d5_0  \n",
      "pyqt5-sip                 12.13.0         py312h5eee18b_0  \n",
      "pysocks                   1.7.1           py312h06a4308_0  \n",
      "python                    3.12.7               h5148396_0  \n",
      "python-dateutil           2.9.0post0      py312h06a4308_2  \n",
      "python-fastjsonschema     2.16.2          py312h06a4308_0  \n",
      "python-json-logger        2.0.7           py312h06a4308_0  \n",
      "python-tzdata             2023.3             pyhd3eb1b0_0  \n",
      "pytz                      2024.1          py312h06a4308_0  \n",
      "pyyaml                    6.0.2           py312h5eee18b_0  \n",
      "pyzmq                     25.1.2          py312h6a678d5_0  \n",
      "qt-main                   5.15.2              h53bd1ea_10  \n",
      "qtconsole                 5.6.0           py312h06a4308_0  \n",
      "qtpy                      2.4.1           py312h06a4308_0  \n",
      "readline                  8.2                  h5eee18b_0  \n",
      "referencing               0.30.2          py312h06a4308_0  \n",
      "requests                  2.32.3          py312h06a4308_0  \n",
      "rfc3339-validator         0.1.4           py312h06a4308_0  \n",
      "rfc3986-validator         0.1.1           py312h06a4308_0  \n",
      "rpds-py                   0.10.6          py312hb02cf49_0  \n",
      "scikit-learn              1.5.1           py312h526ad5a_0  \n",
      "scipy                     1.13.1          py312hc5e2394_0  \n",
      "send2trash                1.8.2           py312h06a4308_0  \n",
      "setuptools                75.1.0          py312h06a4308_0  \n",
      "sip                       6.7.12          py312h6a678d5_0  \n",
      "six                       1.16.0             pyhd3eb1b0_1  \n",
      "sniffio                   1.3.0           py312h06a4308_0  \n",
      "soupsieve                 2.5             py312h06a4308_0  \n",
      "sqlite                    3.45.3               h5eee18b_0  \n",
      "stack_data                0.2.0              pyhd3eb1b0_0  \n",
      "tabulate                  0.9.0           py312h06a4308_0  \n",
      "tbb                       2021.8.0             hdb19cb5_0  \n",
      "terminado                 0.17.1          py312h06a4308_0  \n",
      "threadpoolctl             3.5.0           py312he106c6f_0  \n",
      "tinycss2                  1.2.1           py312h06a4308_0  \n",
      "tk                        8.6.14               h39e8969_0  \n",
      "tornado                   6.4.1           py312h5eee18b_0  \n",
      "traitlets                 5.14.3          py312h06a4308_0  \n",
      "types-python-dateutil     2.9.0.20241003           pypi_0    pypi\n",
      "typing-extensions         4.11.0          py312h06a4308_0  \n",
      "typing_extensions         4.11.0          py312h06a4308_0  \n",
      "tzdata                    2024b                h04d1e81_0  \n",
      "uri-template              1.3.0                    pypi_0    pypi\n",
      "urllib3                   2.2.3           py312h06a4308_0  \n",
      "wcwidth                   0.2.5              pyhd3eb1b0_0  \n",
      "webcolors                 24.8.0                   pypi_0    pypi\n",
      "webencodings              0.5.1           py312h06a4308_2  \n",
      "websocket-client          1.8.0           py312h06a4308_0  \n",
      "wheel                     0.44.0          py312h06a4308_0  \n",
      "widgetsnbextension        4.0.10          py312h06a4308_0  \n",
      "xz                        5.4.6                h5eee18b_1  \n",
      "yaml                      0.2.5                h7b6447c_0  \n",
      "zeromq                    4.3.5                h6a678d5_0  \n",
      "zlib                      1.2.13               h5eee18b_1  \n",
      "zstd                      1.5.6                hc292b87_0  \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59d3b5",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Create the Document-Word matrix</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7278d8-f1b5-47d0-99c3-64e630cae6b7",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Library scikit-learn<a class=\"anchor\" id=\"fourth-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc0134-092d-4d6a-83b1-f02d6c9190c1",
   "metadata": {},
   "source": [
    "Workflow: \n",
    "- Install library\n",
    "- From module import class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915ae7b-97e0-483d-a1f7-0e7c32ce3ea9",
   "metadata": {},
   "source": [
    "Install scikit-learn library. Else, ModuleNotFoundError: No module named 'sklearn'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c71869-58de-4c89-ba19-e5aea99348c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874f561-ed84-45d9-8453-c53051161995",
   "metadata": {},
   "source": [
    "If class is part of a library, it needs to be installed first. Else, NameError: name 'CountVectorizer' is not defined. Classes need to be run in the current session.\n",
    "If stop_words=custom_words, then custom_words need to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40187c7-2fe7-4a8a-a206-a36926e9cc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/smsta/Desktop/zendesk_tag_cloud/zendesk_txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smsta/anaconda3/envs/testenv/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd zendesk_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b848b-e2a4-4dee-b47a-c95b86f1b228",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Class CountVectorizer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee6a0d-afab-43d4-b303-5e491aea420f",
   "metadata": {},
   "source": [
    "Method fit_transform commonly used to simultaneously fit a model/transformer to the data and then transform the data according to that fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859bbc1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_lemmatized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m vectorizer\u001b[38;5;241m=\u001b[39mCountVectorizer(lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,               \u001b[38;5;66;03m#convert all words to lowercase\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                            stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m'\u001b[39m,         \u001b[38;5;66;03m#remove stop words\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                            token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[a-zA-Z]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m3,}\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# only non-digit characters > 3\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m                            \u001b[38;5;66;03m# max_features=50000          # max number of uniq words\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                        )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# this step generates word counts for the words in your docs \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m data_vectorized\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mfit_transform(data_lemmatized)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_lemmatized' is not defined"
     ]
    }
   ],
   "source": [
    "#From module sklearn.feature_extraction.text import class CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer=CountVectorizer(lowercase=True,               #convert all words to lowercase\n",
    "                           stop_words='English',         #remove stop words\n",
    "                           token_pattern='[a-zA-Z]{3,}', # only non-digit characters > 3\n",
    "                           analyzer='word',   \n",
    "                           # min_df=10,                  # minimum reqd occurences of a word\n",
    "                           # max_features=50000          # max number of uniq words\n",
    "                       )\n",
    "    \n",
    "# this step generates word counts for the words in your docs \n",
    "data_vectorized=vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa51b77",
   "metadata": {},
   "source": [
    "6 rows (6 tickets), 54 columns (unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74be924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check rows(docs) and columns(unique words), minus single character words\n",
    "#The columns number is raw word frequency\n",
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a21384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9325c",
   "metadata": {},
   "source": [
    "How many times a word has been used in a ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e6e40",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Count</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2b43d",
   "metadata": {},
   "source": [
    "Get top_n_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count column in Excel spreadsheet\n",
    "np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79258c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "sorted_words_freq =sorted(words_freq, \n",
    "                          key = lambda x: x[1], \n",
    "                          reverse=True)\n",
    "sorted_words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b693101",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(sorted_words_freq[:200],\n",
    "                         columns=['words', 'count'])\n",
    "\n",
    "\n",
    "dataframe.head(201)\n",
    "\n",
    "dataframe.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows in pandas dataframe\n",
    "sliced = dataframe.iloc[[1,2,7,8,9,10,11,13,14,16,19,20,25,26,27,29,30,35,38,39,40,41,49,55,59,64,65,66,67,69,71,76,78,79,80,81,84,87,88,92,93,96,99,104,108,119,126,137,140,146,147,148,149,150,162,164,168,185,186,197], [0,1]]\n",
    "\n",
    "sliced.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4164c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create horizontal barplot\n",
    "ax = sliced.head(10).plot.barh(x='words', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf05be",
   "metadata": {},
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification for count of specific word\n",
    "grep -o -i support *.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52faed58",
   "metadata": {},
   "source": [
    "Got 393559. Check where is the discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000abdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "# Plot horizontal bar graph\n",
    "dataframe.head(10).sort_values(by='count').plot.barh(x='words',\n",
    "                                                     y='count',\n",
    "                                                     ax=ax,\n",
    "                                                     color=\"#a3e8e5\",\n",
    "                                                     fontsize=16,\n",
    "                                                     xlabel='Words')\n",
    "\n",
    "ax.set_title(\"Top ten words in Zendesk tickets\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da59e9d",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Document Frequency</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification for document frequency of a specific word\n",
    "grep -i support *txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b1e19",
   "metadata": {},
   "source": [
    "Got 34654. Check where is the discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f39979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count strings with substring via string list\n",
    "#Verification for document frequency of a specific word\n",
    "subs = 'support'\n",
    "res = len([i for i in all_docs if subs in i]) \n",
    "print (\"All strings count with given substring are : \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count in how many strings within a list a specific substring appears\n",
    "#Verification for document frequency of a specific word\n",
    "substring = \"support\"\n",
    "l = []\n",
    "for x in all_docs:\n",
    "    if substring in x:\n",
    "        l.append(1)\n",
    "    print(substring, len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb330ef",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>TfidfTransformer</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a8dd1",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Smoothed-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "# Plot horizontal bar graph\\n\",\n",
    "dataframe.head(10).sort_values(by='count').plot.barh(x='words',\n",
    "                                                     y='count',\n",
    "                                                     ax=ax,\n",
    "                                                     color=\"#a3e8e5\",\n",
    "                                                     fontsize=16, \n",
    "                                                     xlabel='Words')\n",
    "    \n",
    "ax.set_title(\"Ten most frequent words in Zendesk tickets\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values:\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, \n",
    "                      index=cv.get_feature_names(),\n",
    "                      columns=[\"idf_weights\"]) \n",
    "    \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256fe5d",
   "metadata": {},
   "source": [
    "# <font color='#576675'>TF_IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ac458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix\n",
    "count_vector=cv.transform(all_docs)\n",
    "\n",
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    "    \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    "    \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    " \n",
    "# just send in all your docs here\n",
    "fitted_vectorizer=tfidf_vectorizer.fit(all_docs)\n",
    "tfidf_vectorizer_vectors=fitted_vectorizer.transform(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573582ca",
   "metadata": {},
   "source": [
    "Check the Sparsicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b60502",
   "metadata": {},
   "source": [
    "Build LDA model with sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68f92f",
   "metadata": {},
   "source": [
    "STUDY MORE WHAT IS THE BELOW ABOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61133676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                      )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757da308",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in lda_model.get_topic_terms(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f834e92",
   "metadata": {},
   "source": [
    "10 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447491d",
   "metadata": {},
   "source": [
    "Diagnose model performance with perplexity and log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f7207",
   "metadata": {},
   "source": [
    "Log-likelihood higher the better. Perplexity lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95dc2b",
   "metadata": {},
   "source": [
    "How to GridSearch the best LDA model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f68260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b272c9",
   "metadata": {},
   "source": [
    "How to see the best topic model and its parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ba1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2169b",
   "metadata": {},
   "source": [
    "Compare LDA Model Performance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [10, 15, 20, 25, 30]\n",
    "\n",
    "log_likelyhoods_5 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.9]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc11fc9",
   "metadata": {},
   "source": [
    "How to see the dominant topic in each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ad7e5",
   "metadata": {},
   "source": [
    "Review topics distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e16a8",
   "metadata": {},
   "source": [
    "How to visualize the LDA model with pyLDAvis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483d0fd",
   "metadata": {},
   "source": [
    "Enable the automatic display of visualizations in the IPython Notebook.<br>\n",
    "Transform and prepare a LDA model’s data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf89c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa49b0",
   "metadata": {},
   "source": [
    "pyLDAvis - Python library for interactive topic model visualization. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deaeb92",
   "metadata": {},
   "source": [
    "Save the visualization to a stand-alone HTML file for easy sharing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "p = pyLDAvis.gensim.prepare(best_lda_model, data_vectorized, vectorizer)\n",
    "pyLDAvis.save_html(p, 'lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefb9d9",
   "metadata": {},
   "source": [
    "From lockdown file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50018dd",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>CountVectorizer<a class=\"anchor\" id=\"fourth-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87cc4d",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Create the Document-Word matrix</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer=CountVectorizer(analyzer='word',   \n",
    "                           token_pattern='[a-zA-Z]{3,}', # only non-digit characters > 3\n",
    "                           stop_words=custom_stopwords,  # remove stop words\n",
    "                           lowercase=True,               # convert all words to lowercase\n",
    "                           # min_df=10,                  # minimum reqd occurences of a word\n",
    "                           # max_features=50000,         # max number of uniq words\n",
    "                           \n",
    "                       )\n",
    "    \n",
    "# this step generates word counts for the words in your docs \n",
    "data_vectorized=vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6db4a",
   "metadata": {},
   "source": [
    "6 rows (6 tickets), 54 columns (unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check rows(docs) and columns(unique words), minus single character words\n",
    "#The columns number is raw word frequency\n",
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1dfb92",
   "metadata": {},
   "source": [
    "How many times a word has been used in a ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549defb",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Count</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed889be",
   "metadata": {},
   "source": [
    "Get top_n_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e27743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count column in Excel spreadsheet\n",
    "np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f20d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "sorted_words_freq =sorted(words_freq, \n",
    "                          key = lambda x: x[1], \n",
    "                          reverse=True)\n",
    "sorted_words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b826fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(sorted_words_freq[:200],\n",
    "                         columns=['words', 'count'])\n",
    "\n",
    "\n",
    "dataframe.head(201)\n",
    "\n",
    "dataframe.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows in pandas dataframe\n",
    "sliced = dataframe.iloc[[1,2,5,7,9,10,16,18,19,20,21,22,24,25,26,31,32,33,34,40,43,44,45,46,48,53,54,57,61,62,64,68,70,71,72,77,85,86,92,102,106,108,109,112,124,126,138,140,141,144,147,149,150,152,154,157,158,163,164,167,168,170,176,196,198], [0,1]]\n",
    "\n",
    "sliced.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb27465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create horizontal barplot\n",
    "ax = sliced.head(10).plot.barh(x='words', y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131574f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8086865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
