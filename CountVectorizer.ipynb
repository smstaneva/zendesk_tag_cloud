{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e11cceb",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>CountVectorizer<a class=\"anchor\" id=\"fourth-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a10ffc",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Create the Document-Word matrix</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6480064-7f9e-440b-8019-fe33f38f03e9",
   "metadata": {},
   "source": [
    "List conda environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30f3fe5-4ff9-429d-a16e-5f338ba65376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home/smsta/anaconda3\n",
      "testenv               *  /home/smsta/anaconda3/envs/testenv\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a603fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check installed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e8efb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/smsta/anaconda3/envs/testenv:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main  \n",
      "_openmp_mutex             5.1                       1_gnu  \n",
      "anyio                     4.6.2           py312h06a4308_0  \n",
      "argon2-cffi               21.3.0             pyhd3eb1b0_0  \n",
      "argon2-cffi-bindings      21.2.0          py312h5eee18b_0  \n",
      "arrow                     1.3.0                    pypi_0    pypi\n",
      "asttokens                 2.0.5              pyhd3eb1b0_0  \n",
      "async-lru                 2.0.4           py312h06a4308_0  \n",
      "attrs                     24.2.0          py312h06a4308_0  \n",
      "babel                     2.11.0          py312h06a4308_0  \n",
      "beautifulsoup4            4.12.3          py312h06a4308_0  \n",
      "blas                      1.0                         mkl  \n",
      "bleach                    4.1.0              pyhd3eb1b0_0  \n",
      "bottleneck                1.3.7           py312ha883a20_0  \n",
      "brotli-python             1.0.9           py312h6a678d5_8  \n",
      "bzip2                     1.0.8                h5eee18b_6  \n",
      "ca-certificates           2024.9.24            h06a4308_0  \n",
      "certifi                   2024.8.30       py312h06a4308_0  \n",
      "cffi                      1.17.1          py312h1fdaa30_0  \n",
      "charset-normalizer        3.3.2              pyhd3eb1b0_0  \n",
      "comm                      0.2.1           py312h06a4308_0  \n",
      "cyrus-sasl                2.1.28               h52b45da_1  \n",
      "dbus                      1.13.18              hb2f20db_0  \n",
      "debugpy                   1.6.7           py312h6a678d5_0  \n",
      "decorator                 5.1.1              pyhd3eb1b0_0  \n",
      "defusedxml                0.7.1              pyhd3eb1b0_0  \n",
      "executing                 0.8.3              pyhd3eb1b0_0  \n",
      "expat                     2.6.3                h6a678d5_0  \n",
      "fontconfig                2.14.1               h55d465d_3  \n",
      "fqdn                      1.5.1                    pypi_0    pypi\n",
      "freetype                  2.12.1               h4a9f257_0  \n",
      "glib                      2.78.4               h6a678d5_0  \n",
      "glib-tools                2.78.4               h6a678d5_0  \n",
      "gst-plugins-base          1.14.1               h6a678d5_1  \n",
      "gstreamer                 1.14.1               h5eee18b_1  \n",
      "h11                       0.14.0          py312h06a4308_0  \n",
      "httpcore                  1.0.2           py312h06a4308_0  \n",
      "httpx                     0.27.0          py312h06a4308_0  \n",
      "icu                       73.1                 h6a678d5_0  \n",
      "idna                      3.7             py312h06a4308_0  \n",
      "intel-openmp              2023.1.0         hdb19cb5_46306  \n",
      "ipykernel                 6.29.5          py312h06a4308_0  \n",
      "ipython                   8.27.0          py312h06a4308_0  \n",
      "ipywidgets                8.1.2           py312h06a4308_0  \n",
      "isoduration               20.11.0                  pypi_0    pypi\n",
      "jedi                      0.19.1          py312h06a4308_0  \n",
      "jinja2                    3.1.4           py312h06a4308_0  \n",
      "joblib                    1.4.2           py312h06a4308_0  \n",
      "jpeg                      9e                   h5eee18b_3  \n",
      "json5                     0.9.6              pyhd3eb1b0_0  \n",
      "jsonpointer               3.0.0                    pypi_0    pypi\n",
      "jsonschema                4.23.0          py312h06a4308_0  \n",
      "jsonschema-specifications 2023.7.1        py312h06a4308_0  \n",
      "jupyter                   1.0.0           py312h06a4308_9  \n",
      "jupyter-lsp               2.2.0           py312h06a4308_0  \n",
      "jupyter_client            8.6.0           py312h06a4308_0  \n",
      "jupyter_console           6.6.3           py312h06a4308_1  \n",
      "jupyter_core              5.7.2           py312h06a4308_0  \n",
      "jupyter_events            0.10.0          py312h06a4308_0  \n",
      "jupyter_server            2.14.1          py312h06a4308_0  \n",
      "jupyter_server_terminals  0.4.4           py312h06a4308_1  \n",
      "jupyterlab                4.2.5           py312h06a4308_0  \n",
      "jupyterlab_pygments       0.1.2                      py_0  \n",
      "jupyterlab_server         2.27.3          py312h06a4308_0  \n",
      "jupyterlab_widgets        3.0.10          py312h06a4308_0  \n",
      "krb5                      1.20.1               h143b758_1  \n",
      "ld_impl_linux-64          2.40                 h12ee557_0  \n",
      "libclang                  14.0.6          default_hc6dbbc7_1  \n",
      "libclang13                14.0.6          default_he11475f_1  \n",
      "libcups                   2.4.2                h2d74bed_1  \n",
      "libedit                   3.1.20230828         h5eee18b_0  \n",
      "libffi                    3.4.4                h6a678d5_1  \n",
      "libgcc-ng                 11.2.0               h1234567_1  \n",
      "libgfortran-ng            11.2.0               h00389a5_1  \n",
      "libgfortran5              11.2.0               h1234567_1  \n",
      "libglib                   2.78.4               hdc74915_0  \n",
      "libgomp                   11.2.0               h1234567_1  \n",
      "libiconv                  1.16                 h5eee18b_3  \n",
      "libllvm14                 14.0.6               hecde1de_4  \n",
      "libpng                    1.6.39               h5eee18b_0  \n",
      "libpq                     12.17                hdbd6064_0  \n",
      "libsodium                 1.0.18               h7b6447c_0  \n",
      "libstdcxx-ng              11.2.0               h1234567_1  \n",
      "libuuid                   1.41.5               h5eee18b_0  \n",
      "libxcb                    1.15                 h7f8727e_0  \n",
      "libxkbcommon              1.0.1                h097e994_2  \n",
      "libxml2                   2.13.1               hfdd30dd_2  \n",
      "lz4-c                     1.9.4                h6a678d5_1  \n",
      "markupsafe                2.1.3           py312h5eee18b_0  \n",
      "matplotlib-inline         0.1.6           py312h06a4308_0  \n",
      "mistune                   2.0.4           py312h06a4308_0  \n",
      "mkl                       2023.1.0         h213fc3f_46344  \n",
      "mkl-service               2.4.0           py312h5eee18b_1  \n",
      "mkl_fft                   1.3.10          py312h5eee18b_0  \n",
      "mkl_random                1.2.7           py312h526ad5a_0  \n",
      "mysql                     5.7.24               h721c034_2  \n",
      "nbclient                  0.8.0           py312h06a4308_0  \n",
      "nbconvert                 7.16.4          py312h06a4308_0  \n",
      "nbformat                  5.10.4          py312h06a4308_0  \n",
      "ncurses                   6.4                  h6a678d5_0  \n",
      "nest-asyncio              1.6.0           py312h06a4308_0  \n",
      "notebook                  7.2.2           py312h06a4308_1  \n",
      "notebook-shim             0.2.3           py312h06a4308_0  \n",
      "numexpr                   2.10.1          py312h3c60e43_0  \n",
      "numpy                     1.26.4          py312hc5e2394_0  \n",
      "numpy-base                1.26.4          py312h0da6c21_0  \n",
      "openssl                   3.0.15               h5eee18b_0  \n",
      "overrides                 7.4.0           py312h06a4308_0  \n",
      "packaging                 24.1            py312h06a4308_0  \n",
      "pandas                    2.2.2           py312h526ad5a_0  \n",
      "pandocfilters             1.5.0              pyhd3eb1b0_0  \n",
      "parso                     0.8.3              pyhd3eb1b0_0  \n",
      "pcre2                     10.42                hebb0a14_1  \n",
      "pexpect                   4.8.0              pyhd3eb1b0_3  \n",
      "pip                       24.2            py312h06a4308_0  \n",
      "platformdirs              3.10.0          py312h06a4308_0  \n",
      "ply                       3.11            py312h06a4308_1  \n",
      "prometheus_client         0.14.1          py312h06a4308_0  \n",
      "prompt-toolkit            3.0.43          py312h06a4308_0  \n",
      "prompt_toolkit            3.0.43               hd3eb1b0_0  \n",
      "psutil                    5.9.0           py312h5eee18b_0  \n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2  \n",
      "pure_eval                 0.2.2              pyhd3eb1b0_0  \n",
      "pybind11-abi              5                    hd3eb1b0_0  \n",
      "pycparser                 2.21               pyhd3eb1b0_0  \n",
      "pygments                  2.15.1          py312h06a4308_1  \n",
      "pyqt                      5.15.10         py312h6a678d5_0  \n",
      "pyqt5-sip                 12.13.0         py312h5eee18b_0  \n",
      "pysocks                   1.7.1           py312h06a4308_0  \n",
      "python                    3.12.7               h5148396_0  \n",
      "python-dateutil           2.9.0post0      py312h06a4308_2  \n",
      "python-fastjsonschema     2.16.2          py312h06a4308_0  \n",
      "python-json-logger        2.0.7           py312h06a4308_0  \n",
      "python-tzdata             2023.3             pyhd3eb1b0_0  \n",
      "pytz                      2024.1          py312h06a4308_0  \n",
      "pyyaml                    6.0.2           py312h5eee18b_0  \n",
      "pyzmq                     25.1.2          py312h6a678d5_0  \n",
      "qt-main                   5.15.2              h53bd1ea_10  \n",
      "qtconsole                 5.6.0           py312h06a4308_0  \n",
      "qtpy                      2.4.1           py312h06a4308_0  \n",
      "readline                  8.2                  h5eee18b_0  \n",
      "referencing               0.30.2          py312h06a4308_0  \n",
      "requests                  2.32.3          py312h06a4308_0  \n",
      "rfc3339-validator         0.1.4           py312h06a4308_0  \n",
      "rfc3986-validator         0.1.1           py312h06a4308_0  \n",
      "rpds-py                   0.10.6          py312hb02cf49_0  \n",
      "scikit-learn              1.5.1           py312h526ad5a_0  \n",
      "scipy                     1.13.1          py312hc5e2394_0  \n",
      "send2trash                1.8.2           py312h06a4308_0  \n",
      "setuptools                75.1.0          py312h06a4308_0  \n",
      "sip                       6.7.12          py312h6a678d5_0  \n",
      "six                       1.16.0             pyhd3eb1b0_1  \n",
      "sniffio                   1.3.0           py312h06a4308_0  \n",
      "soupsieve                 2.5             py312h06a4308_0  \n",
      "sqlite                    3.45.3               h5eee18b_0  \n",
      "stack_data                0.2.0              pyhd3eb1b0_0  \n",
      "tabulate                  0.9.0           py312h06a4308_0  \n",
      "tbb                       2021.8.0             hdb19cb5_0  \n",
      "terminado                 0.17.1          py312h06a4308_0  \n",
      "threadpoolctl             3.5.0           py312he106c6f_0  \n",
      "tinycss2                  1.2.1           py312h06a4308_0  \n",
      "tk                        8.6.14               h39e8969_0  \n",
      "tornado                   6.4.1           py312h5eee18b_0  \n",
      "traitlets                 5.14.3          py312h06a4308_0  \n",
      "types-python-dateutil     2.9.0.20241003           pypi_0    pypi\n",
      "typing-extensions         4.11.0          py312h06a4308_0  \n",
      "typing_extensions         4.11.0          py312h06a4308_0  \n",
      "tzdata                    2024b                h04d1e81_0  \n",
      "uri-template              1.3.0                    pypi_0    pypi\n",
      "urllib3                   2.2.3           py312h06a4308_0  \n",
      "wcwidth                   0.2.5              pyhd3eb1b0_0  \n",
      "webcolors                 24.8.0                   pypi_0    pypi\n",
      "webencodings              0.5.1           py312h06a4308_2  \n",
      "websocket-client          1.8.0           py312h06a4308_0  \n",
      "wheel                     0.44.0          py312h06a4308_0  \n",
      "widgetsnbextension        4.0.10          py312h06a4308_0  \n",
      "xz                        5.4.6                h5eee18b_1  \n",
      "yaml                      0.2.5                h7b6447c_0  \n",
      "zeromq                    4.3.5                h6a678d5_0  \n",
      "zlib                      1.2.13               h5eee18b_1  \n",
      "zstd                      1.5.6                hc292b87_0  \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092675c",
   "metadata": {},
   "source": [
    "Install scikit-learn library. Else, ModuleNotFoundError: No module named 'sklearn'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ca88cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/smsta/anaconda3/envs/testenv\n",
      "\n",
      "  added / updated specs:\n",
      "    - scikit-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    joblib-1.4.2               |  py312h06a4308_0         513 KB\n",
      "    libgfortran-ng-11.2.0      |       h00389a5_1          20 KB\n",
      "    libgfortran5-11.2.0        |       h1234567_1         2.0 MB\n",
      "    pybind11-abi-5             |       hd3eb1b0_0          14 KB\n",
      "    scikit-learn-1.5.1         |  py312h526ad5a_0        10.3 MB\n",
      "    scipy-1.13.1               |  py312hc5e2394_0        23.7 MB\n",
      "    threadpoolctl-3.5.0        |  py312he106c6f_0          49 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        36.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  joblib             pkgs/main/linux-64::joblib-1.4.2-py312h06a4308_0\n",
      "  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-11.2.0-h00389a5_1\n",
      "  libgfortran5       pkgs/main/linux-64::libgfortran5-11.2.0-h1234567_1\n",
      "  pybind11-abi       pkgs/main/noarch::pybind11-abi-5-hd3eb1b0_0\n",
      "  scikit-learn       pkgs/main/linux-64::scikit-learn-1.5.1-py312h526ad5a_0\n",
      "  scipy              pkgs/main/linux-64::scipy-1.13.1-py312hc5e2394_0\n",
      "  threadpoolctl      pkgs/main/linux-64::threadpoolctl-3.5.0-py312he106c6f_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "libgfortran-ng-11.2. | 20 KB     | ##################################### | 100% \n",
      "joblib-1.4.2         | 513 KB    | ##################################### | 100% \n",
      "pybind11-abi-5       | 14 KB     | ##################################### | 100% \n",
      "scipy-1.13.1         | 23.7 MB   | ##################################### | 100% \n",
      "threadpoolctl-3.5.0  | 49 KB     | ##################################### | 100% \n",
      "libgfortran5-11.2.0  | 2.0 MB    | ##################################### | 100% \n",
      "scikit-learn-1.5.1   | 10.3 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9418f",
   "metadata": {},
   "source": [
    "From module sklearn.feature_extraction import class CountVectorizer. If class is part of a library, it needs to be imported first. Else, NameError: name 'CountVectorizer' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f097416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712a6fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Convert a collection of text documents to a matrix of token counts\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vectorizer\u001b[38;5;241m=\u001b[39mCountVectorizer(analyzer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m,   \n\u001b[1;32m      3\u001b[0m                            token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[a-zA-Z]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m3,}\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# only non-digit characters > 3\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m                            stop_words\u001b[38;5;241m=\u001b[39mcustom_stopwords,  \u001b[38;5;66;03m# remove stop words\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                            lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,               \u001b[38;5;66;03m# convert all words to lowercase\u001b[39;00m\n\u001b[1;32m      6\u001b[0m                            \u001b[38;5;66;03m# min_df=10,                  # minimum reqd occurences of a word\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                            \u001b[38;5;66;03m# max_features=50000,         # max number of uniq words\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                            \n\u001b[1;32m      9\u001b[0m                        )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# this step generates word counts for the words in your docs \u001b[39;00m\n\u001b[1;32m     12\u001b[0m data_vectorized\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mfit_transform(data_lemmatized)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer=CountVectorizer(analyzer='word',   \n",
    "                           token_pattern='[a-zA-Z]{3,}', # only non-digit characters > 3\n",
    "                           stop_words=custom_stopwords,  # remove stop words\n",
    "                           lowercase=True,               # convert all words to lowercase\n",
    "                           # min_df=10,                  # minimum reqd occurences of a word\n",
    "                           # max_features=50000,         # max number of uniq words\n",
    "                           \n",
    "                       )\n",
    "    \n",
    "# this step generates word counts for the words in your docs \n",
    "data_vectorized=vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d572580",
   "metadata": {},
   "source": [
    "6 rows (6 tickets), 54 columns (unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check rows(docs) and columns(unique words), minus single character words\n",
    "#The columns number is raw word frequency\n",
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c017474",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0374f",
   "metadata": {},
   "source": [
    "How many times a word has been used in a ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac2ba9",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Count</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc7e61",
   "metadata": {},
   "source": [
    "Get top_n_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327af7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count column in Excel spreadsheet\n",
    "np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc159eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc866fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "sorted_words_freq =sorted(words_freq, \n",
    "                          key = lambda x: x[1], \n",
    "                          reverse=True)\n",
    "sorted_words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(sorted_words_freq[:200],\n",
    "                         columns=['words', 'count'])\n",
    "\n",
    "\n",
    "dataframe.head(201)\n",
    "\n",
    "dataframe.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows in pandas dataframe\n",
    "sliced = dataframe.iloc[[1,2,7,8,9,10,11,13,14,16,19,20,25,26,27,29,30,35,38,39,40,41,49,55,59,64,65,66,67,69,71,76,78,79,80,81,84,87,88,92,93,96,99,104,108,119,126,137,140,146,147,148,149,150,162,164,168,185,186,197], [0,1]]\n",
    "\n",
    "sliced.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50300798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create horizontal barplot\n",
    "ax = sliced.head(10).plot.barh(x='words', y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f809ee9",
   "metadata": {},
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification for count of specific word\n",
    "grep -o -i support *.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21c7ea",
   "metadata": {},
   "source": [
    "Got 393559. Check where is the discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "# Plot horizontal bar graph\n",
    "dataframe.head(10).sort_values(by='count').plot.barh(x='words',\n",
    "                                                     y='count',\n",
    "                                                     ax=ax,\n",
    "                                                     color=\"#a3e8e5\",\n",
    "                                                     fontsize=16,\n",
    "                                                     xlabel='Words')\n",
    "\n",
    "ax.set_title(\"Top ten words in Zendesk tickets\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8fd78",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Document Frequency</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification for document frequency of a specific word\n",
    "grep -i support *txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f25d9b",
   "metadata": {},
   "source": [
    "Got 34654. Check where is the discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count strings with substring via string list\n",
    "#Verification for document frequency of a specific word\n",
    "subs = 'support'\n",
    "res = len([i for i in all_docs if subs in i]) \n",
    "print (\"All strings count with given substring are : \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count in how many strings within a list a specific substring appears\n",
    "#Verification for document frequency of a specific word\n",
    "substring = \"support\"\n",
    "l = []\n",
    "for x in all_docs:\n",
    "    if substring in x:\n",
    "        l.append(1)\n",
    "    print(substring, len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d6225",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>TfidfTransformer</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f57b8f",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Smoothed-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a395c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "# Plot horizontal bar graph\\n\",\n",
    "dataframe.head(10).sort_values(by='count').plot.barh(x='words',\n",
    "                                                     y='count',\n",
    "                                                     ax=ax,\n",
    "                                                     color=\"#a3e8e5\",\n",
    "                                                     fontsize=16, \n",
    "                                                     xlabel='Words')\n",
    "    \n",
    "ax.set_title(\"Ten most frequent words in Zendesk tickets\", fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89585f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values:\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, \n",
    "                      index=cv.get_feature_names(),\n",
    "                      columns=[\"idf_weights\"]) \n",
    "    \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218c9f6",
   "metadata": {},
   "source": [
    "# <font color='#576675'>TF_IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix\n",
    "count_vector=cv.transform(all_docs)\n",
    "\n",
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b90d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    "    \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    "    \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8586cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881902f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    " \n",
    "# just send in all your docs here\n",
    "fitted_vectorizer=tfidf_vectorizer.fit(all_docs)\n",
    "tfidf_vectorizer_vectors=fitted_vectorizer.transform(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65faf50a",
   "metadata": {},
   "source": [
    "Check the Sparsicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f99a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb85b5d",
   "metadata": {},
   "source": [
    "Build LDA model with sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258c63a",
   "metadata": {},
   "source": [
    "STUDY MORE WHAT IS THE BELOW ABOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b34407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                      )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in lda_model.get_topic_terms(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3817f92",
   "metadata": {},
   "source": [
    "10 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8777640",
   "metadata": {},
   "source": [
    "Diagnose model performance with perplexity and log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df6949",
   "metadata": {},
   "source": [
    "Log-likelihood higher the better. Perplexity lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15541cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceeff67",
   "metadata": {},
   "source": [
    "How to GridSearch the best LDA model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c29c16",
   "metadata": {},
   "source": [
    "How to see the best topic model and its parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d7bf9b",
   "metadata": {},
   "source": [
    "Compare LDA Model Performance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [10, 15, 20, 25, 30]\n",
    "\n",
    "log_likelyhoods_5 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.5]\n",
    "log_likelyhoods_7 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.7]\n",
    "log_likelyhoods_9 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.9]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdd313",
   "metadata": {},
   "source": [
    "How to see the dominant topic in each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a02f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9f7ef",
   "metadata": {},
   "source": [
    "Review topics distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbdc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc49fb4",
   "metadata": {},
   "source": [
    "How to visualize the LDA model with pyLDAvis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcad198",
   "metadata": {},
   "source": [
    "Enable the automatic display of visualizations in the IPython Notebook.<br>\n",
    "Transform and prepare a LDA model’s data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2939e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e0eac",
   "metadata": {},
   "source": [
    "pyLDAvis - Python library for interactive topic model visualization. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c6d67",
   "metadata": {},
   "source": [
    "Save the visualization to a stand-alone HTML file for easy sharing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "p = pyLDAvis.gensim.prepare(best_lda_model, data_vectorized, vectorizer)\n",
    "pyLDAvis.save_html(p, 'lda.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634aeb62",
   "metadata": {},
   "source": [
    "From lockdown file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe3a65",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>CountVectorizer<a class=\"anchor\" id=\"fourth-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58b80e",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Create the Document-Word matrix</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer=CountVectorizer(analyzer='word',   \n",
    "                           token_pattern='[a-zA-Z]{3,}', # only non-digit characters > 3\n",
    "                           stop_words=custom_stopwords,  # remove stop words\n",
    "                           lowercase=True,               # convert all words to lowercase\n",
    "                           # min_df=10,                  # minimum reqd occurences of a word\n",
    "                           # max_features=50000,         # max number of uniq words\n",
    "                           \n",
    "                       )\n",
    "    \n",
    "# this step generates word counts for the words in your docs \n",
    "data_vectorized=vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8db60",
   "metadata": {},
   "source": [
    "6 rows (6 tickets), 54 columns (unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883853f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check rows(docs) and columns(unique words), minus single character words\n",
    "#The columns number is raw word frequency\n",
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fe9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ca54e",
   "metadata": {},
   "source": [
    "How many times a word has been used in a ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c78b3",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Count</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975cd30",
   "metadata": {},
   "source": [
    "Get top_n_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count column in Excel spreadsheet\n",
    "np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8930cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "sorted_words_freq =sorted(words_freq, \n",
    "                          key = lambda x: x[1], \n",
    "                          reverse=True)\n",
    "sorted_words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(sorted_words_freq[:200],\n",
    "                         columns=['words', 'count'])\n",
    "\n",
    "\n",
    "dataframe.head(201)\n",
    "\n",
    "dataframe.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows in pandas dataframe\n",
    "sliced = dataframe.iloc[[1,2,5,7,9,10,16,18,19,20,21,22,24,25,26,31,32,33,34,40,43,44,45,46,48,53,54,57,61,62,64,68,70,71,72,77,85,86,92,102,106,108,109,112,124,126,138,140,141,144,147,149,150,152,154,157,158,163,164,167,168,170,176,196,198], [0,1]]\n",
    "\n",
    "sliced.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de38d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create horizontal barplot\n",
    "ax = sliced.head(10).plot.barh(x='words', y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a39f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e8aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
