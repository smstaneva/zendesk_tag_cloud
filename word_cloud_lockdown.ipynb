{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Preprocessing<a class=\"anchor\" id=\"second-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "pwd   #Display the path of current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = sorted(glob.glob('/home/smsta/Desktop/GIT_REPOSITORIES/zendesk_tag_cloud/newman_lockdown/*.json'))\n",
    "for filename in file_list:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE",
     "IJSON"
    ]
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "def parse_json(json_filename):\n",
    "        with open(filename, 'r', encoding=\"utf8\") as file:\n",
    "            # load json iteratively\n",
    "            parser = ijson.parse(file)\n",
    "            for prefix, event, value in parser:\n",
    "                print('prefix={}, event={}, value={}'.format(prefix, event, value))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parse_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "from string import punctuation\n",
    "def extract_ticket_text_generator(json_filename):\n",
    "    \"\"\"This function takes a list of files with tickets and extracts text from each ticket. The result is a list of text strings.\"\"\"\n",
    "    for filename in file_list:\n",
    "            with open(filename, 'r', encoding=\"utf8\") as input_file:\n",
    "                # Extract specific items from the file\n",
    "                tickets = ijson.items(input_file, 'run.executions.item.assertions.item.assertion')\n",
    "                for ticket in tickets:\n",
    "                    # Extract the substring between two markers\n",
    "                    l = re.findall('plain_body(.+?)public', ticket)\n",
    "                    #Remove escaped newline '\\\\n' and non-breaking space 'nbsp' characters\n",
    "                    m = [re.sub(r'\\\\n|nbsp', ' ', t) for t in l]\n",
    "                    # Remove any URL within a string\n",
    "                    p = [re.sub(r'http\\S+|www\\S+', '', o) for o in m]          \n",
    "                    # Remove all of the punctuation in any item in the list. The result is for each ticket a list of comments.\n",
    "                    q = [''.join(c for c in s if c not in punctuation) for s in p]\n",
    "                    # Join list elements without any separator. The result is for each ticket a list of merged comments.\n",
    "                    r = [' '.join(q)] \n",
    "                    yield(r)\n",
    "                \n",
    "    if __name__ == '__main__':\n",
    "        extract_ticket_text_generator(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "def create_txt_files():\n",
    "        \"\"\"This function takes a list of text strings and saves each ticket in a .txt file.\"\"\"\n",
    "        data = extract_ticket_text_generator(filename)\n",
    "        # Make a flat list out of list of lists.\n",
    "        flat_list = [item for sublist in data for item in sublist]\n",
    "        for i in range(len(flat_list)):\n",
    "            with open(\"ticket_%d.txt\" % (i+59047), 'w', encoding=\"utf-8\") as f:\n",
    "                f.write(flat_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "create_txt_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside of project scope - create a .txt file with all tickets in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tickets_all_txt_file():\n",
    "        \"\"\"This function takes a list of text strings and saves all tickets in a .txt file.\"\"\"\n",
    "        data = extract_ticket_text_generator(filename)\n",
    "        # Make a flat list out of list of lists.\n",
    "        flat_list = [item for sublist in data for item in sublist]\n",
    "        with open('ticket_all_lockdown.txt', 'w', encoding=\"utf-8\") as filehandle:\n",
    "            #Save all elements of a list as a text file:\n",
    "            for listitem in flat_list:\n",
    "                filehandle.write('%s\\n\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tickets_all_txt_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside of project scope above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "    \n",
    "all_txt_files =[]\n",
    "for file in Path(\"zendesk_txt_lockdown\").rglob(\"*.txt\"):\n",
    "    all_txt_files.append(file.parent / file.name)\n",
    "    # counts the length of the list\n",
    "    n_files = len(all_txt_files)\n",
    "    print(n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "DONE"
    ]
   },
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for txt_file in all_txt_files:\n",
    "    with open(txt_file, encoding=\"utf-8\") as f:\n",
    "        txt_file_as_string = f.read()\n",
    "        all_docs.append(txt_file_as_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>Unigram WordCloud<a class=\"anchor\" id=\"third-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Load the packages:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "WORKING"
    ]
   },
   "outputs": [],
   "source": [
    "# Run in terminal or command prompt\n",
    "\n",
    "import gensim\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk\n",
    "\n",
    "#Wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Tokenize and Clean-up using gensim’s simple_preprocess()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "DONE",
     "GENSIM"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['event', 'query', 'anthony', 'valente', 'technical', 'support', 'specialist', 'eventsforce', 'solutions', 'ltd', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'hi', 'matt', 'it', 'was', 'great', 'speaking', 'with', 'you', 'earlier', 'as', 'we', 'discussed', 'you', 'should', 'certainly', 'be', 'able', 'to', 'accommodate', 'your', 'event', 'requirements', 'in', 'eventsforce', 'wanted', 'to', 'provide', 'some', 'links', 'to', 'the', 'data', 'security', 'policy', 'and', 'others', 'please', 'see', 'below', 'privacy', 'and', 'security', 'policy', 'service', 'level', 'agreement', 'data', 'security', 'policy', 'eventsforce', 'app', 'licence', 'terms', 'and', 'eventsforce', 'virtual', 'content', 'delivery', 'vcd', 'data', 'security', 'policy', 'securitypolicy', 'please', 'let', 'us', 'know', 'if', 'you', 'have', 'any', 'questions', 'thanks', 'anthony', 'valente', 'technical', 'support', 'specialist', 'eventsforce', 'solutions', 'ltd', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'cheers', 'anthony', 'appreciate', 'your', 'time', 'today', 'and', 'hopeful', 'this', 'is', 'solution', 'we', 'can', 'use', 'matt', 'matt', 'taylerson', 'porsche', 'centre', 'marketing', 'service', 'tbpthe', 'blueprint', 'partners', 'ltd', 'disraeli', 'road', 'london', 'sw', 'dz', 'tel', 'fax', 'mob', 'email', 'this', 'message', 'was', 'sent', 'from', 'an', 'individual', 'provided', 'by', 'the', 'blueprint', 'partners', 'ltd', 'working', 'in', 'partnership', 'with', 'porsche', 'cars', 'great', 'britain', 'ltd', 'porsche', 'centre', 'marketing', 'service', 'is', 'trading', 'name', 'used', 'under', 'licence', 'from', 'porsche', 'cars', 'great', 'britain', 'ltd', 'tbpthe', 'blueprint', 'partners', 'ltd', 'company', 'registered', 'in', 'england', 'number', 'registered', 'office', 'disraeli', 'road', 'london', 'sw', 'dz', 'vat', 'registration', 'any', 'opinions', 'expressed', 'in', 'this', 'email', 'are', 'those', 'of', 'the', 'individual', 'and', 'not', 'necessarily', 'the', 'company', 'the', 'information', 'in', 'this', 'message', 'is', 'confidential', 'and', 'may', 'be', 'privileged', 'it', 'is', 'intended', 'for', 'the', 'addressee', 'alone', 'if', 'you', 'are', 'not', 'the', 'intended', 'recipient', 'it', 'is', 'prohibited', 'to', 'disclose', 'use', 'or', 'copy', 'this', 'information', 'please', 'contact', 'the', 'sender', 'immediately', 'should', 'this', 'message', 'have', 'been', 'transmitted', 'incorrectly', 'this', 'email', 'and', 'any', 'attachments', 'have', 'been', 'scanned', 'for', 'the', 'presence', 'of', 'computer', 'viruses', 'the', 'blueprint', 'partners', 'ltd', 'accept', 'no', 'responsibility', 'for', 'computer', 'viruses', 'once', 'this', 'email', 'has', 'been', 'transmitted', 'please', 'consider', 'the', 'environment', 'before', 'printing', 'this', 'email'], ['cant', 'reset', 'password', 'after', 'following', 'reset', 'password', 'link', 'the', 'person', 'has', 'used', 'the', 'system', 'before', 'but', 'cant', 'remember', 'their', 'password', 'this', 'ticket', 'has', 'been', 'assigned', 'by', 'round', 'robin', 'hi', 'rebecca', 'thank', 'you', 'for', 'your', 'message', 'which', 'event', 'is', 'this', 'for', 'and', 'who', 'is', 'the', 'delegate', 'just', 'so', 'can', 'check', 'the', 'email', 'that', 'has', 'gone', 'out', 'to', 'them', 'thanks', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'thanks', 'kim', 'for', 'your', 'reply', 'the', 'event', 'is', 'ncasc', 'virtual', 'registration', 'contact', 'is', 'thanks', 'rebecca', 'thanks', 'rebecca', 'have', 'had', 'look', 'at', 'this', 'event', 'and', 'he', 'has', 'not', 'been', 'sent', 'password', 'reset', 'email', 'from', 'this', 'event', 'he', 'might', 'have', 'been', 'using', 'an', 'old', 'one', 'and', 'the', 'link', 'would', 'have', 'been', 'expired', 'or', 'used', 'different', 'email', 'address', 'could', 'you', 'ask', 'him', 'to', 'go', 'to', 'the', 'event', 'enter', 'his', 'email', 'and', 'then', 'request', 'the', 'email', 'again', 'or', 'you', 'could', 'even', 'do', 'it', 'for', 'him', 'but', 'he', 'will', 'have', 'to', 'click', 'on', 'the', 'link', 'within', 'minutes', 'or', 'use', 'one', 'of', 'your', 'personal', 'links', 'in', 'that', 'email', 'please', 'let', 'me', 'know', 'if', 'you', 'need', 'any', 'more', 'assistance', 'with', 'this', 'kind', 'regards', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'hi', 'rebecca', 'hope', 'you', 'are', 'well', 'am', 'just', 'following', 'up', 'to', 'see', 'if', 'you', 'need', 'any', 'more', 'assistance', 'with', 'this', 'as', 'never', 'heard', 'back', 'form', 'you', 'kind', 'regards', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'thanks', 'kim', 'not', 'quite', 'sure', 'we', 've', 'managed', 'to', 'resolve', 'the', 'issue', 'yet', 'll', 'be', 'back', 'in', 'touch', 'if', 'need', 'additional', 'help', 'thanks', 'rebecca', 'thanks', 'for', 'letting', 'me', 'know', 'rebecca', 'would', 'you', 'like', 'me', 'to', 'keep', 'this', 'ticket', 'open', 'or', 'shall', 'close', 'it', 'and', 'you', 'can', 'then', 'reopen', 'it', 'if', 'you', 'do', 'need', 'more', 'assistance', 'with', 'it', 'kind', 'regards', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'please', 'keep', 'the', 'ticket', 'open', 'for', 'now', 'thanks', 'rebecca', 'sure', 'will', 'do', 'thanks', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'hi', 'rebecca', 'just', 'checking', 'if', 'you', 'managed', 'to', 'resolve', 'this', 'now', 'thanks', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'good', 'morning', 'rebecca', 'would', 'you', 'need', 'any', 'more', 'assistance', 'with', 'this', 'before', 'close', 'the', 'ticket', 'kind', 'regards', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'hi', 'rebecca', 'as', 'have', 'not', 'heard', 'back', 'from', 'you', 'will', 'close', 'this', 'ticket', 'for', 'now', 'but', 'please', 'feel', 'free', 'to', 'reopen', 'by', 'replying', 'to', 'this', 'message', 'if', 'you', 'would', 'need', 'more', 'assistance', 'with', 'this', 'have', 'great', 'weekend', 'kind', 'regards', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web', 'hi', 'kim', 'believe', 'this', 'problem', 'has', 'now', 'been', 'resolved', 'please', 'close', 'the', 'ticket', 'thanks', 'rebecca', 'great', 'thanks', 'for', 'letting', 'me', 'know', 'rebecca', 'kind', 'regards', 'kim', 'kim', 'de', 'vries', 'technical', 'support', 'team', 'leader', 'eventsforce', 'solutions', 'ltd', 'working', 'days', 'wednesday', 'friday', 'uk', 'support', 'us', 'support', 'apac', 'support', 'web']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(all_docs))\n",
    "\n",
    "print(data_words[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Lemmatization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "SPACY",
     "DONE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['event support support support web great speak early discuss should certainly able accommodate event requirement eventsforce want provide link datum security policy other see privacy security policy service level agreement datum security policy licence term eventsforce virtual content delivery datum security policy securitypolicy let know question thank support support support web cheer appreciate time today hopeful solution can use mob email message send individual provide work partnership car trade name use licence car register register registration opinion express email individual necessarily company information message confidential may privileged intend addressee alone intended recipient prohibit disclose use copy information contact sender immediately should message transmit incorrectly email attachment scan presence computer virus partner accept responsibility computer virus email transmit consider environment print email', 'can reset password follow reset password link person use system before can remember password ticket assign thank message event delegate just so can check email go thank technical support team leader day support support support web thank reply event ncasc virtual registration contact thank thank look event send reset email event may use old one link would expire use different email address could ask go event enter email then request email again could even will click link minute use personal link email let know need more assistance kind regard technical support team leader day support support support web hope well just follow see need more assistance never hear back form kind regard technical support team leader day support support support web thank quite sure ve manage resolve issue yet will back touch need additional help thank rebecca thank let know rebecca would like keep ticket open shall close can then reopen need more assistance kind regard technical support team leader day support support apac support web keep ticket open now thank sure will thank technical support team leader day support support support web just check manage resolve now thank technical support team leader day support support support good morning rebecca would need more assistance close ticket kind regard technical support team leader day support support support web hi rebecca hear back will close ticket now feel free reopen reply message would need more assistance great weekend kind regard technical support team leader day support support support web believe problem now resolve close ticket thank rebecca great thank let know regard technical support team leader day support support support web']\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom list of English stopwords\n",
    "custom_stopwords = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords = custom_stopwords,\n",
    "                      background_color=\"white\",\n",
    "                      width = 4000,\n",
    "                      height = 2000,\n",
    "                      max_words=200, \n",
    "                      collocations = False,   #remove repetitive words\n",
    "                      min_word_length = 3, \n",
    "                         ).generate(''.join(data_lemmatized))\n",
    "    \n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "wordcloud.to_file(\"word_cloud_lockdown_unigrams.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = ' '.join(data_lemmatized)\n",
    "print(new_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list1 = []\n",
    "for words in new_list: \n",
    "    new_list1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_list1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(new_list)\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#31708f'><center>CountVectorizer<a class=\"anchor\" id=\"fourth-bullet\"></a></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Create the Document-Word matrix</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of text documents to a matrix of token counts\n",
    "vectorizer=CountVectorizer(analyzer='word',   \n",
    "                           token_pattern='[a-zA-Z]{3,}', # only non-digit characters > 3\n",
    "                           stop_words=custom_stopwords,  # remove stop words\n",
    "                           lowercase=True,               # convert all words to lowercase\n",
    "                           # min_df=10,                  # minimum reqd occurences of a word\n",
    "                           # max_features=50000,         # max number of uniq words\n",
    "                           \n",
    "                       )\n",
    "    \n",
    "# this step generates word counts for the words in your docs \n",
    "data_vectorized=vectorizer.fit_transform(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 rows (6 tickets), 54 columns (unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check rows(docs) and columns(unique words), minus single character words\n",
    "#The columns number is raw word frequency\n",
    "data_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times a word has been used in a ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#576675'>Count</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top_n_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count column in Excel spreadsheet\n",
    "np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = np.asarray(data_vectorized.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "sorted_words_freq =sorted(words_freq, \n",
    "                          key = lambda x: x[1], \n",
    "                          reverse=True)\n",
    "sorted_words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(sorted_words_freq[:200],\n",
    "                         columns=['words', 'count'])\n",
    "\n",
    "\n",
    "dataframe.head(201)\n",
    "\n",
    "dataframe.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows in pandas dataframe\n",
    "sliced = dataframe.iloc[[1,2,5,7,9,10,16,18,19,20,21,22,24,25,26,31,32,33,34,40,43,44,45,46,48,53,54,57,61,62,64,68,70,71,72,77,85,86,92,102,106,108,109,112,124,126,138,140,141,144,147,149,150,152,154,157,158,163,164,167,168,170,176,196,198], [0,1]]\n",
    "\n",
    "sliced.style.set_properties(subset=['words', 'count'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create horizontal barplot\n",
    "ax = sliced.head(10).plot.barh(x='words', y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
